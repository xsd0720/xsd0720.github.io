<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Audio | 码农人生]]></title>
  <link href="http://msching.github.io/blog/categories/audio/atom.xml" rel="self"/>
  <link href="http://msching.github.io/"/>
  <updated>2016-07-01T17:50:20+08:00</updated>
  <id>http://msching.github.io/</id>
  <author>
    <name><![CDATA[ChengYinZju]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[iOS音频播放 (九)：边播边缓存]]></title>
    <link href="http://msching.github.io/blog/2016/05/24/audio-in-ios-9/"/>
    <updated>2016-05-24T09:46:48+08:00</updated>
    <id>http://msching.github.io/blog/2016/05/24/audio-in-ios-9</id>
    <content type="html"><![CDATA[<p>好久没写过博客了，在这期间有很多同学通过博客评论、微博私信、邮件和我交流iOS音频方面的相关问题，其中被问到最多的是如何实现“边播边缓存”，这篇就来说一说这个话题。顺便一提，本文的题目虽然为“iOS音频播放”，但其中所涉及的部分技术方案在OSX平台或者在流播放视频下同样适用。</p>

<!--more-->


<hr />

<h1>我能想到的方案</h1>

<p>这类的技术方案其实有不少（其实在<a href="/blog/2014/07/07/audio-in-ios/">第一篇</a>的末尾也略微有所涉及）：</p>

<p>思路1. 最直接的方式，自行实现音频数据的请求在请求的过程中把数据缓存到磁盘，然后基于磁盘的数据自己实现解码、播放等功能；这个方法作为直接也最为复杂，开发者需要对音频播放的原理、操作系统等知识有一定程度的理解。如果能够实现这种方式所达到的效果也将会是最好的，整个过程都由开发者掌控，出现问题也可以对症下药。开源播放器<a href="https://github.com/muhku/FreeStreamer">FreeStreamer</a>就是一个很好的例子，使用带有cache功能开源播放器或在其基础上进行二次开发也是不错的选择；</p>

<p>思路2. 请求拦截的方式，首先你需要一个能够进行流播放的播放器（如Apple提供的<a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVPlayer_Class/">AVPlayer</a>），通过拦截播放器发送的请求可以知道需要下载哪一段数据，于是就可以根据本地缓存文件的情况分段为播放器提供数据，如遇到已缓存的数据段直接从缓存中获取数据塞回给播放器，如遇到未缓存的数据段就发送请求获取数据，得到response和数据后保存到磁盘同时塞回给播放器。这种思路下有三个分支：</p>

<p>思路2.1 流播放器 + LocalServer，首先在搭建一个LocalServer（例如使用<a href="https://github.com/swisspol/GCDWebServer">GCDWebServer</a>），然后将URL组织成类似这种形式：</p>

<blockquote><p><a href="http://localhost:port?url=urlEncode">http://localhost:port?url=urlEncode</a>(audioUrl)</p></blockquote>

<p>把组织好的URL交个播放器播放，播放器把请求发送到LocalServer上，LocalServer解析到实际的音频地址后发送请求或者读取已缓存的数据。</p>

<p>思路2.2 流播放器 + NSURLProtocol，大家都知道<a href="https://developer.apple.com/library/mac/documentation/Cocoa/Reference/Foundation/Classes/NSURLProtocol_Class/">NSURLProtocol</a>可以拦截Cocoa平台下<a href="https://developer.apple.com/library/mac/documentation/Cocoa/Conceptual/URLLoadingSystem/URLLoadingSystem.html">URL Loading System</a>中的请求，如果播放器的请求是运行在<code>URL Loading System</code>下的话使用这个方法可以轻松的拦截到播放器所发送的请求然后自己再进行请求或者读取缓存数据。这里需要注意如果使用AVPlayer作为播放器的话这种方法只在模拟器上才work，真机上并不能拦截到任何请求。这也证明AVPlayer在真机上并没有运行在<code>URL Loading System</code>下，但模拟器上却是（不知道在OSX下是否能work，有兴趣的同学可以尝试一下）。</p>

<p>注：如果播放器使用的是CFNetwork，也可以尝试拦截，例如使用FB的<a href="https://github.com/facebook/fishhook">fishhook</a>，这hook方法应该会遇上不少坑，请做好心理准备。。</p>

<p>思路2.3 AVPlayer + AVAssetResourceLoader，<a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetResourceLoader_Class/">AVAssetResourceLoader</a>是iOS 6之后添加的类其主要作用是让开发者能够掌控<code>AVURLAsset</code>加载数据的整个过程。这正好符合我们的需求，<code>AVAssetResourceLoader</code>会通过delegate把<code>AVAssetResourceLoadingRequest</code>对象传递给开发者，开发者可以根据其中的一些属性得知需要加载的数据段。在得到数据后也可以通过<code>AVAssetResourceLoadingRequest</code>向AVPlayer传递response和数据。</p>

<p>思路3. 取巧的方式，自行实现音频数据的请求在请求的过程中把数据缓存到磁盘，然后使用系统提供的播放器（如AVAudioPlayer、AVPlayer进行播放）。这种实现方式中需要注意的是要播放的音频文件需要预先缓存一定量之后才能够播放，具体缓存多少完全频个人感觉，并且有可能会产生播放失败或者播放错误。这种方式的另一个缺点是无法进行任意的seek；</p>

<hr />

<h1>方案的选择</h1>

<p>上面提到了3种思路共5个方案，那么在实际开发过程中开发者应该可以根据各个方案的优劣结合自己的实际情况选择最适合自己的方案。</p>

<p>思路1：优点在于整个播放过程可控，出现问题可调试，但开发复杂度较高，故选择有对应功能的开源播放器是一个比较好途径。在使用开源播放器之前最好能阅读其代码，掌握整个播放流程，出了问题才能迅速定位。推荐以播放为核心功能的app使用此方案；</p>

<p>思路2：优点在于开发者不必关心播放的整个过程，对音频播放的相关知识也不必有太多的了解，整个开发过程只要关心请求的解析、缓存数据的读取和保存以及数据的回填即可；至于缺点，首先你的有一个靠谱的流播放器，如果使用AVPlayer那么请做踩坑准备；</p>

<p>思路2.1：各类流播放器通吃，如果方案2.2和2.3不管用2.1是最好的选择；</p>

<p>思路2.2：需要播放器有指定的请求方式，如运行在<code>URL Loading System</code>下；</p>

<p>思路2.3：如果你用的就是AVPlayer那么可以尝试使用这个思路，但对于播放列表形式(M3U8)的音频这种方式是无效的；</p>

<p>思路3：如果你选择这条路，那说明你真的懒得不行。。。</p>

<hr />

<h1>思路2缓存和数据读取细节</h1>

<p>一般音频流或者视频流都会支持HTTP协议中的<a href="https://en.wikipedia.org/wiki/List_of_HTTP_header_fields#range-request-header">Range request header</a>，所以大多数的流播放器都会对<code>Range header</code>进行支持，在数据源支持Range的情况下拦截到请求时有必要注意播放器所请求的数据段并根据当前数据缓存的状态进行分段处理。</p>

<p>举个例子，播放器请求bytes=0-100，其中10-20、50-60已经被缓存，那么这个请求就应该被分为下面几段来处理：</p>

<ol>
<li>0-10，网络请求</li>
<li>10-20，本地缓存</li>
<li>20-50，网络请求</li>
<li>50-60，本地缓存</li>
<li>60-100，网络请求</li>
</ol>


<p>以上几段数据请求按顺序执行并进行数据回填，其中通过网络请求的数据在收到之后加入缓存以便下一次请求再次使用。另外要注意的是由于播放器本身只发送了一个请求所以response还是只有一个并且Content-Range还是应该为<code>0-100/FileLength</code>。</p>

<hr />

<h1>AVAssetResourceLoader踩坑</h1>

<div class="github-card" data-github="msching/AVPlayerCacheSupport" data-width="400" data-height="" data-theme="default"></div>


<script src="//cdn.jsdelivr.net/github-cards/latest/widget.js"></script>


<p><a href="https://github.com/msching/AVPlayerCacheSupport">AVPlayerCacheSupport</a>是我使用<code>AVAssetResourceLoader</code>进行实践后实现的一个开源项目，在开发的过程中踩到的坑也在这里分享给大家。</p>

<h3>shceme必须自定义</h3>

<p>非自定义的URL Scheme不会触发AVAssetResourceLoader的delegate方法。这一点并不难发现，Stackoverflow上和github上都有提到这一点。所以在构造AVPlayItem时必须使用自定义Scheme的URL才行，这里我是在原有的Scheme后加上了<code>-streaming</code>，在收到AVAssetResourceLoader的回调之后实际发送请求时再把<code>-streaming</code>后缀去掉。</p>

<h3>AVURLAsset.resourceLoader的delegate必须在AVPlayerItem生成前赋值</h3>

<p>看代码感受一下吧，这样写能接到回调：</p>

<p><code>objc
AVURLAsset *asset = [AVURLAsset URLAssetWithURL:url] options:options];
[asset.resourceLoader setDelegate:self queue:dispatch_get_main_queue()];
AVPlayerItem *item = [self playerItemWithAsset:asset];
</code>
下面这种写法是无法接到回调的：</p>

<p><code>objc
AVURLAsset *asset = [AVURLAsset URLAssetWithURL:url] options:options];
AVPlayerItem *item = [self playerItemWithAsset:asset];
[[(AVURLAsset *)item.asset resourceLoader] setDelegate:self queue:dispatch_get_main_queue()];
</code></p>

<h3>不支持Playlist类型的播放</h3>

<p>AVAssetResourceLoader不支持类似M3U和M3U8这类播放列表类型的流，这个问题的回答来自<a href="http://stackoverflow.com/a/30239876">SO链接</a>，<a href="https://developer.apple.com/library/ios/technotes/tn2232/_index.html#//apple_ref/doc/uid/DTS40012884-CH1-SECHTTPLIVESTREAMING">官方文档</a>中关于HTTP Live Streaming的一段话也印证了这一点。</p>

<p>在搜索相关问题之前，我尝试了使用AVAssetResourceLoader去加载M3U8播放列表，其中M3U8文件可以获取到，但并非获取了之后直接存储就完事了，还需要进行一些处理：</p>

<p>M3U8中一般有两种类型的URL：相对地址的URL和绝对地址的URL，其中相对地址的URL不需要处理AVPlayer会根据原先的host（也就是带了-streaming后缀的host）进行请求，这样的请求还是会被AVAssetResourceLoader拦截到。而绝对地址的URL则需要对其中的scheme进行处理使其能够被AVAssetResourceLoader拦截。</p>

<p>处理完所有的URL以后才能把M3U8文件进行保存。</p>

<p>M3U8处理完成之后，就尝试处理其中的一些媒体文件地址，例如ts格式的视频，但经过尝试后发现这类ts的链接并不能被AVAssetResourceLoader拦截到，这才去搜索相关内容后找到了上述的SO链接和官方文档。</p>

<h3>AVAssetResourceLoadingContentInformationRequest的contentLength和contentType</h3>

<p><code>AVAssetResourceLoadingContentInformationRequest</code>是<code>AVAssetResourceLoadingRequest</code>的一个属性</p>

<p><code>objc
/*!
 @property      contentInformationRequest
 @abstract      An instance of AVAssetResourceLoadingContentInformationRequest that you should populate with information about the resource. The value of this property will be nil if no such information is being requested.
*/
@property (nonatomic, readonly, nullable) AVAssetResourceLoadingContentInformationRequest *contentInformationRequest NS_AVAILABLE(10_9, 7_0);
</code></p>

<p>其作用是告诉AVPlayer当前加载的资源类型、文件大小等信息。</p>

<p><code>AVAssetResourceLoadingContentInformationRequest</code>有这样一个属性：</p>

<p><code>
/*!
 @property      contentLength
 @abstract      Indicates the length of the requested resource, in bytes.
 @discussion    Before you finish loading an AVAssetResourceLoadingRequest, if its contentInformationRequest is not nil, you should set the value of this property to the number of bytes contained by the requested resource.
*/
@property (nonatomic) long long contentLength;
</code>
乍看上去可以把当前所请求数据的<code>Content-Length</code>直接赋给这个属性，例如请求range=0-100的那么其<code>Content-Length</code>就是100。如果当前数据无缓存的话，就直接把<code>NSURLResponse</code>的<code>expectedContentLength</code>属性值赋值给了contentLength。</p>

<p>但经过实践发现上面的做法并不正确。对于支持<code>Range</code>的请求，如range=0-100，<code>NSURLResponse</code>的<code>expectedContentLength</code>属性值为100，但这里需要填入的是文件的总长。所以对于response header中包含<code>Content-Range</code>的请求，需要解析出其中的文件总长再赋值给<code>AVAssetResourceLoadingContentInformationRequest</code>的<code>contentLength</code>属性。</p>

<p>接下来是<code>contentType</code>：</p>

<p><code>objc
/*!
 @property      contentType
 @abstract      A UTI that indicates the type of data contained by the requested resource.
 @discussion    Before you finish loading an AVAssetResourceLoadingRequest, if its contentInformationRequest is not nil, you should set the value of this property to a UTI indicating the type of data contained by the requested resource.
*/
@property (nonatomic, copy, nullable) NSString *contentType;
</code></p>

<p>这里的<code>contentType</code>是UTI，和<code>NSURLResponse</code>的<code>MIMEType</code>并不相同。需要进行转换：</p>

<p>```objc
NSString *mimeType = [response MIMEType];
CFStringRef contentType = UTTypeCreatePreferredIdentifierForTag(kUTTagClassMIMEType, (__bridge CFStringRef)(mimeType), NULL);
loadingRequest.contentInformationRequest.contentType = CFBridgingRelease(contentType);</p>

<h2>```</h2>

<h1>总结</h1>

<p>要说的就这么多，希望能帮到大家 =)。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iOS音频播放 (八)：NowPlayingCenter和RemoteControl]]></title>
    <link href="http://msching.github.io/blog/2014/11/06/audio-in-ios-8/"/>
    <updated>2014-11-06T13:26:28+08:00</updated>
    <id>http://msching.github.io/blog/2014/11/06/audio-in-ios-8</id>
    <content type="html"><![CDATA[<p>距离<a href="/blog/2014/09/07/audio-in-ios-7/">上一篇</a>博文发布已经有一个月多的时间了，在这其间我一直忙于筹办婚礼以至于这篇博文一直拖到了现在。</p>

<p>在之前<a href="/blog/categories/ios-audio/">一到六篇</a>中我对iOS下的音频播放流程进行了阐述，在<a href="/blog/2014/09/07/audio-in-ios-7/">第七篇</a>中介绍了如何播放iPod Lib中的歌曲，至此有关音频播放的话题就已经完结了，在这篇里我将会讲到的<code>NowPlayingCenter</code>和<code>RemoteControl</code>这两个玩意本身和整个播放流程并没有什么关系，但它们可以让音频播放在iOS系统上获得更加好的用户体验。</p>

<!--more-->


<hr />

<h1>NowPlayingCenter</h1>

<p><code>NowPlayingCenter</code>能够显示当前正在播放的歌曲信息，它可以控制的范围包括：</p>

<ul>
<li>锁频界面上所显示的歌曲播放信息和图片</li>
<li>iOS7之后控制中心上显示的歌曲播放信息</li>
<li>iOS7之前双击home键后出现的进程中向左滑动出现的歌曲播放信息</li>
<li>AppleTV，AirPlay中显示的播放信息</li>
<li>车载系统中显示的播放信息</li>
</ul>


<p>这些信息的显示都由<code>MPNowPlayingInfoCenter</code>类来控制，这个类的定义非常简单：</p>

<p>```objc
MP_EXTERN_CLASS_AVAILABLE(5_0) @interface MPNowPlayingInfoCenter : NSObject</p>

<p>// Returns the default now playing info center.
// The default center holds now playing info about the current application.
+ (MPNowPlayingInfoCenter *)defaultCenter;</p>

<p>// The current now playing info for the center.
// Setting the info to nil will clear it.
@property (copy) NSDictionary *nowPlayingInfo;</p>

<p>@end
```</p>

<p>使用也同样简单，首先<code>#import &lt;MediaPlayer/MPNowPlayingInfoCenter.h&gt;</code>然后调用<code>MPNowPlayingInfoCenter</code>的单例方法获取实例，再把需要显示的信息组织成Dictionary并赋值给<code>nowPlayingInfo</code>属性就完成了。</p>

<p><code>nowPlayingInfo</code>中一些常用属性被定义在<code>&lt;MediaPlayer/MPMediaItem.h&gt;</code>中</p>

<p><code>objc
MPMediaItemPropertyAlbumTitle                //NSString
MPMediaItemPropertyAlbumTrackCount          //NSNumber of NSUInteger
MPMediaItemPropertyAlbumTrackNumber     //NSNumber of NSUInteger
MPMediaItemPropertyArtist                   //NSString
MPMediaItemPropertyArtwork                  //MPMediaItemArtwork
MPMediaItemPropertyComposer             //NSString
MPMediaItemPropertyDiscCount                //NSNumber of NSUInteger
MPMediaItemPropertyDiscNumber               //NSNumber of NSUInteger
MPMediaItemPropertyGenre                    //NSString
MPMediaItemPropertyPersistentID         //NSNumber of uint64_t
MPMediaItemPropertyPlaybackDuration     //NSNumber of NSTimeInterval
MPMediaItemPropertyTitle                    //NSString
</code></p>

<p>上面这些属性大多比较浅显易懂，基本上按照字面上的意思去理解就可以了，需要稍微解释以下的是<code>MPMediaItemPropertyArtwork</code>。这个属性表示的是锁屏界面或者AirPlay中显示的歌曲封面图，<code>MPMediaItemArtwork</code>类可以由<code>UIImage</code>类进行初始化。</p>

<p>```objc
MP_EXTERN_CLASS_AVAILABLE(3_0) @interface MPMediaItemArtwork : NSObject</p>

<p>// Initializes an MPMediaItemArtwork instance with the given full-size image.
// The crop rect of the image is assumed to be equal to the bounds of the
// image as defined by the image&rsquo;s size in points, i.e. tightly cropped.
&ndash; (instancetype)initWithImage:(UIImage *)image NS_DESIGNATED_INITIALIZER NS_AVAILABLE_IOS(5_0);</p>

<p>// Returns the artwork image for an item at a given size (in points).
&ndash; (UIImage *)imageWithSize:(CGSize)size;</p>

<p>@property (nonatomic, readonly) CGRect bounds; // The bounds of the full size image (in points).
@property (nonatomic, readonly) CGRect imageCropRect; // The actual content area of the artwork, in the bounds of the full size image (in points).</p>

<p>@end
```</p>

<p>另外一些附加属性被定义在<code>&lt;MediaPlayer/MPNowPlayingInfoCenter.h&gt;</code>中</p>

<p>```objc
// The elapsed time of the now playing item, in seconds.
// Note the elapsed time will be automatically extrapolated from the previously
// provided elapsed time and playback rate, so updating this property frequently
// is not required (or recommended.)
MP_EXTERN NSString *const MPNowPlayingInfoPropertyElapsedPlaybackTime NS_AVAILABLE_IOS(5_0); // NSNumber (double)</p>

<p>// The playback rate of the now playing item, with 1.0 representing normal
// playback. For example, 2.0 would represent playback at twice the normal rate.
// If not specified, assumed to be 1.0.
MP_EXTERN NSString *const MPNowPlayingInfoPropertyPlaybackRate NS_AVAILABLE_IOS(5_0); // NSNumber (double)</p>

<p>// The &ldquo;default&rdquo; playback rate of the now playing item. You should set this
// property if your app is playing a media item at a rate other than 1.0 in a
// default playback state. e.g., if you are playing back content at a rate of
// 2.0 and your playback state is not fast-forwarding, then the default
// playback rate should also be 2.0. Conversely, if you are playing back content
// at a normal rate (1.0) but the user is fast-forwarding your content at a rate
// greater than 1.0, then the default playback rate should be set to 1.0.
MP_EXTERN NSString *const MPNowPlayingInfoPropertyDefaultPlaybackRate NS_AVAILABLE_IOS(8_0); // NSNumber (double)</p>

<p>// The index of the now playing item in the application&rsquo;s playback queue.
// Note that the queue uses zero-based indexing, so the index of the first item
// would be 0 if the item should be displayed as &ldquo;item 1 of 10&rdquo;.
MP_EXTERN NSString *const MPNowPlayingInfoPropertyPlaybackQueueIndex NS_AVAILABLE_IOS(5_0); // NSNumber (NSUInteger)</p>

<p>// The total number of items in the application&rsquo;s playback queue.
MP_EXTERN NSString *const MPNowPlayingInfoPropertyPlaybackQueueCount NS_AVAILABLE_IOS(5_0); // NSNumber (NSUInteger)</p>

<p>// The chapter currently being played. Note that this is zero-based.
MP_EXTERN NSString *const MPNowPlayingInfoPropertyChapterNumber NS_AVAILABLE_IOS(5_0); // NSNumber (NSUInteger)</p>

<p>// The total number of chapters in the now playing item.
MP_EXTERN NSString *const MPNowPlayingInfoPropertyChapterCount NS_AVAILABLE_IOS(5_0); // NSNumber (NSUInteger)
<code>``
其中常用的是</code>MPNowPlayingInfoPropertyElapsedPlaybackTime<code>和</code>MPNowPlayingInfoPropertyPlaybackRate`：</p>

<ul>
<li><code>MPNowPlayingInfoPropertyElapsedPlaybackTime</code>表示已经播放的时间，用这个属性可以让<code>NowPlayingCenter</code>显示播放进度；</li>
<li><code>MPNowPlayingInfoPropertyPlaybackRate</code>表示播放速率。通常情况下播放速率为1.0，即真是时间的1秒对应播放时间中的1秒；</li>
</ul>


<p>这里需要解释的是，<code>NowPlayingCenter</code>中的进度刷新并不是由app不停的更新<code>nowPlayingInfo</code>来做的，而是根据app传入的<code>ElapsedPlaybackTime</code>和<code>PlaybackRate</code>进行自动刷新。例如传入ElapsedPlaybackTime=120s，PlaybackRate=1.0，那么<code>NowPlayingCenter</code>会显示2:00并且在接下来的时间中每一秒把进度加1秒并刷新显示。如果需要暂停进度，传入PlaybackRate=0.0即可。</p>

<p>所以每次播放暂停和继续都需要更新<code>NowPlayingCenter</code>并正确设置<code>ElapsedPlaybackTime</code>和<code>PlaybackRate</code>否则<code>NowPlayingCenter</code>中的播放进度无法正常显示。</p>

<h3>NowPlayingCenter的刷新时机</h3>

<p>频繁的刷新<code>NowPlayingCenter</code>并不可取，特别是在有Artwork的情况下。所以需要在合适的时候进行刷新。</p>

<p>依照我自己的经验下面几个情况下刷新<code>NowPlayingCenter</code>比较合适：</p>

<ul>
<li>当前播放歌曲进度被拖动时</li>
<li>当前播放的歌曲变化时</li>
<li>播放暂停或者恢复时</li>
<li>当前播放歌曲的信息发生变化时（例如Artwork，duration等）</li>
</ul>


<p>在刷新时可以适当的通过判断app是否active来决定是否必须刷新以减少刷新次数。</p>

<h3>MPMediaItemPropertyArtwork</h3>

<p>这是一个非常有用的属性，我们可以利用歌曲的封面图来合成一些图片借此达到美化锁屏界面或者显示锁屏歌词。</p>

<hr />

<h1>RemoteControl</h1>

<p><code>RemoteComtrol</code>可以用来在不打开app的情况下控制app中的多媒体播放行为，涉及的内容主要包括：</p>

<ul>
<li>锁屏界面双击Home键后出现的播放操作区域</li>
<li>iOS7之后控制中心的播放操作区域</li>
<li>iOS7之前双击home键后出现的进程中向左滑动出现的播放操作区域</li>
<li>AppleTV，AirPlay中显示的播放操作区域</li>
<li>耳机线控</li>
<li>车载系统的设置</li>
</ul>


<h3>在何处处理RemoteComtrol</h3>

<p>根据<a href="https://developer.apple.com/library/ios/documentation/EventHandling/Conceptual/EventHandlingiPhoneOS/Remote-ControlEvents/Remote-ControlEvents.html">官方文档</a>的描述：</p>

<p><code>If your app plays audio or video content, you might want it to respond to remote control events that originate from either transport controls or external accessories. (External accessories must conform to Apple-provided specifications.) iOS converts commands into UIEvent objects and delivers the events to an app. The app sends them to the first responder and, if the first responder doesn’t handle them, they travel up the responder chain.</code></p>

<p>当<code>RemoteComtrol</code>事件产生时，iOS会以<code>UIEvent</code>的形式发送给app，app会首先转发到first responder，如果first responder不处理这个事件的话那么事件就会沿着responder chain继续转发。关于responder chain的相关内容可以查看<a href="https://developer.apple.com/library/IOs/documentation/General/Conceptual/Devpedia-CocoaApp/Responder.html">这里</a>。</p>

<p>从responder chain文档看来如果之前的所有responder全部不响应<code>RemoteComtrol</code>事件的话，最终事件会被转发给Application（如图）。所以我们知道作为responder chain的最末端，在<code>UIApplication</code>中实现<code>RemoteComtrol</code>的处理是最为合理的，而并非在UIWindow中或者AppDelegate中。</p>

<p><img src="/images/iOS-audio/responder%20chain.jpg" alt="" /></p>

<h3>实现自己的UIApplication</h3>

<p>首先新建一个<code>UIApplication</code>的子类</p>

<p>```objc</p>

<h1>import &lt;UIKit/UIKit.h></h1>

<p>@interface MyApplication : UIApplication</p>

<p>@end
```</p>

<p>然后找到工程中的<code>main.m</code>，可以看到代码如下：</p>

<p>```c
int main(int argc, char * argv[])
{</p>

<pre><code>@autoreleasepool {
    return UIApplicationMain(argc, argv, nil, NSStringFromClass([AppDelegate class]));
}
</code></pre>

<p>}
<code>``
在main中调用了</code>UIApplicationMain`方法</p>

<p><code>objc
// If nil is specified for principalClassName, the value for NSPrincipalClass from the Info.plist is used. If there is no
// NSPrincipalClass key specified, the UIApplication class is used. The delegate class will be instantiated using init.
UIKIT_EXTERN int UIApplicationMain(int argc, char *argv[], NSString *principalClassName, NSString *delegateClassName);
</code></p>

<p>我们需要做的就是给<code>UIApplicationMain</code>方法的第三个参数传入我们的application类名，如下：</p>

<p>```c</p>

<h1>import &lt;UIKit/UIKit.h></h1>

<h1>import &ldquo;AppDelegate.h&rdquo;</h1>

<h1>import &ldquo;MyApplication.h&rdquo;</h1>

<p>int main(int argc, char * argv[])
{</p>

<pre><code>@autoreleasepool {
    return UIApplicationMain(argc, argv, NSStringFromClass([MyApplication class]), NSStringFromClass([AppDelegate class]));
}
</code></pre>

<p>}
```</p>

<p>这样就成功实现了自己的<code>UIApplication</code>.</p>

<h3>处理RemoteComtrol</h3>

<p>了解了应该在何处处理<code>RemoteComtrol</code>事件之后，再来看下<a href="https://developer.apple.com/library/ios/documentation/EventHandling/Conceptual/EventHandlingiPhoneOS/Remote-ControlEvents/Remote-ControlEvents.html">官方文档</a>中描述的三个必要条件：</p>

<ul>
<li>接受者必须能够成为first responder</li>
<li>必须显示地声明接收<code>RemoteComtrol</code>事件</li>
<li>你的app必须是<code>Now Playing</code>app</li>
</ul>


<p>对于第一条就是要在自己的<code>UIApplication</code>中实现<code>canBecomeFirstResponder</code>方法:</p>

<p>```objc</p>

<h1>import &ldquo;MyApplication.h&rdquo;</h1>

<p>@implementation MyApplication</p>

<ul>
<li>(BOOL)canBecomeFirstResponder
{
  return YES;
}</li>
</ul>


<p>@end
<code>``
第二条是要求显示地调用</code>[[UIApplication sharedApplication] beginReceivingRemoteControlEvents]`，调用的实际一般是在播放开始时；</p>

<p>第三条就是要求占据NowPlayingCenter，这个之前已经提到过了。</p>

<p>满足三个条件后可以在<code>UIApplication</code>中实现处理<code>RemoteComtrol</code>事件的方法，根据不同的事件实现不同的操作即可。</p>

<p>```objc</p>

<h1>import &ldquo;MyApplication.h&rdquo;</h1>

<p>@implementation MyApplication</p>

<ul>
<li><p>(BOOL)canBecomeFirstResponder
{
  return YES;
}</p></li>
<li><p>(void)remoteControlReceivedWithEvent:(UIEvent *)event
{
  switch (event.subtype)
  {
      case UIEventSubtypeRemoteControlPlay:
            //play
          break;
      case UIEventSubtypeRemoteControlPause:
            //pause
          break;
      case UIEventSubtypeRemoteControlStop:
            //stop
          break;
      default:
          break;
  }
}</p></li>
</ul>


<p>@end
```</p>

<hr />

<h1>示例代码</h1>

<p>git上有一个关于remotecontrol的小工程供大家参考<a href="https://github.com/MosheBerman/ios-audio-remote-control">ios-audio-remote-control</a></p>

<div class="github-card" data-github="MosheBerman/ios-audio-remote-control" data-width="400" data-height="" data-theme="default"></div>


<script src="//cdn.jsdelivr.net/github-cards/latest/widget.js"></script>


<hr />

<h1>后记</h1>

<p>到本篇为止iOS的音频播放话题基本上算是完结了。接下来我会在空余时间去研究一下iOS 8中新加入的<code>AVAudioEngine</code>，其功能涵盖播放、录音、混音、音效处理，看上去十分强大，从接口的定义上看像是对<code>AudioUnit</code>的高层封装，当研究有了一定的成果之后也会以博文的形式分享出来。</p>

<hr />

<h1>参考资料</h1>

<p><a href="https://encrypted.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0CC4QFjAA&amp;url=https%3A%2F%2Fdeveloper.apple.com%2FLibrary%2Fios%2Fdocumentation%2FMediaPlayer%2FReference%2FMPNowPlayingInfoCenter_Class%2Findex.html&amp;ei=8bBhVO_RF4HKmwXBiILIDA&amp;usg=AFQjCNFOziF2zKft-wGQ3ew_cHy7Ivxrvg">MPNowPlayingInfoCenter</a></p>

<p><a href="https://developer.apple.com/library/ios/documentation/EventHandling/Conceptual/EventHandlingiPhoneOS/Remote-ControlEvents/Remote-ControlEvents.html">Remote Control Events</a></p>

<p><a href="https://developer.apple.com/library/IOs/documentation/General/Conceptual/Devpedia-CocoaApp/Responder.html">Cocoa Responder Chain</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iOS音频播放 (七)：播放iPod Library中的歌曲]]></title>
    <link href="http://msching.github.io/blog/2014/09/07/audio-in-ios-7/"/>
    <updated>2014-09-07T15:45:47+08:00</updated>
    <id>http://msching.github.io/blog/2014/09/07/audio-in-ios-7</id>
    <content type="html"><![CDATA[<p>由于最近工作量非常饱和，所以这第七篇来的有点晚（创建时间是9月7日。。说出来都是泪）。</p>

<p>现在市面上的音乐播放器都支持iPod Library歌曲（俗称iPod音乐或者本地音乐）的播放，用户对于iPod音乐播放的需求也一直十分强烈。这篇要讲的是如何来播放iPod Library的歌曲。</p>

<!--more-->


<hr />

<h1>概述</h1>

<p>根据<a href="https://developer.apple.com/library/ios/documentation/audiovideo/conceptual/multimediapg/usingaudio/usingaudio.html#//apple_ref/doc/uid/TP40009767-CH2-SW43">官方文档</a>描述Apple从iOS 3.0开始允许开发者访问用户的iPod library来获取用户放在其中的歌曲等多媒体内容。</p>

<p>为此Apple提供了多种方法来访问和播放iPod中的音乐，下面我们来分别列举一下这些方法。</p>

<hr />

<h1>访问MediaLibrary</h1>

<p><a href="https://developer.apple.com/library/ios/documentation/Audio/Conceptual/iPodLibraryAccess_Guide/AboutiPodLibraryAccess/AboutiPodLibraryAccess.html#//apple_ref/doc/uid/TP40008765-CH103-SW9">官方文档</a>访问iPod Library的方法有两种，分别是MediaPicker和MediaQuery。</p>

<p><img src="/images/iOS-audio/iPodLibraryAccessOverview.jpg" alt="" /></p>

<h3>MediaPicker</h3>

<p>MediaPicker是一个高度封装的iPod Library访问方式，通过使用<code>MPMediaPickerController</code>类来访问iPod Library。这是一个UI控件，用户可以根据需要选择其中的音乐。这个类使用时非常方便，只需要生成一个``的实例，设置一下属性和delegate后present出来，接下来只要等待回调即可，在回调时需要手动dismiss picker。</p>

<p>```objc
MPMediaPickerController *picker = [[MPMediaPickerController alloc] initWithMediaTypes:MPMediaTypeAnyAudio];
picker.prompt = @&ldquo;请选择需要播放的歌曲&rdquo;;
picker.showsCloudItems = NO;
picker.allowsPickingMultipleItems = YES;
picker.delegate = self;
[self presentViewController:picker animated:YES completion:nil];</p>

<ul>
<li><p>(void)mediaPickerDidCancel:(MPMediaPickerController *)mediaPicker
{
  [mediaPicker dismissViewControllerAnimated:YES completion:nil];
}</p></li>
<li><p>(void)mediaPicker:(MPMediaPickerController <em>)mediaPicker didPickMediaItems:(MPMediaItemCollection </em>)mediaItemCollection
{
  [mediaPicker dismissViewControllerAnimated:YES completion:nil];
  //do something
}
```</p></li>
</ul>


<p>上面的代码将会得到如下的效果：</p>

<p><img src="/images/iOS-audio/MediaPicker.jpg" alt="" /></p>

<p>通过MediaPicker最终可以得到<code>MPMediaItemCollection</code>，其中存放着所有在Picker中选中的歌曲，每一个歌曲使用一个<code>MPMediaItem</code>对象表示。对于MediaPicker的使用也可以参考<a href="https://developer.apple.com/library/ios/documentation/Audio/Conceptual/iPodLibraryAccess_Guide/UsingtheMediaItemPicker/UsingtheMediaItemPicker.html#//apple_ref/doc/uid/TP40008765-CH104-SW1">官方文档</a>。</p>

<h3>MediaQuery</h3>

<p>如果你觉得MeidaPicker的功能或者UI不能满足你的要求那么可以使用MediaQuery。MediaQuery可以直接访问iPod Library的DB，并根据需要获取数据。<a href="https://developer.apple.com/library/ios/documentation/Audio/Conceptual/iPodLibraryAccess_Guide/UsingTheiPodLibrary/UsingTheiPodLibrary.html#//apple_ref/doc/uid/TP40008765-CH101-SW1">官方文档</a>给出了MediaQuery的示意图。</p>

<p><img src="/images/iOS-audio/database_access_classes.jpg" alt="" /></p>

<p>MediaQuery功能十分强大，它可以根据一个或多个条件查询满足需要的MediaItem。</p>

<p>你可以使用<code>MPMediaQuery</code>的类方法来生成一些已经预置了条件的Query</p>

<p><code>objc
// Base queries which can be used directly or as the basis for custom queries.
// The groupingType for these queries is preset to the appropriate type for the query.
+ (MPMediaQuery *)albumsQuery;
+ (MPMediaQuery *)artistsQuery;
+ (MPMediaQuery *)songsQuery;
+ (MPMediaQuery *)playlistsQuery;
+ (MPMediaQuery *)podcastsQuery;
+ (MPMediaQuery *)audiobooksQuery;
+ (MPMediaQuery *)compilationsQuery;
+ (MPMediaQuery *)composersQuery;
+ (MPMediaQuery *)genresQuery;
</code>
也可以自己生成<code>MPMediaPredicate</code>设置条件，并把它加到Query中，最后通过items和collections访问查询到的结果，例如：</p>

<p>```objc
MPMediaPropertyPredicate *artistNamePredicate =
[MPMediaPropertyPredicate predicateWithValue:@&ldquo;Happy the Clown&rdquo;</p>

<pre><code>                             forProperty:MPMediaItemPropertyArtist
                          comparisonType:MPMediaPredicateComparisonEqualTo];
</code></pre>

<p>MPMediaQuery *quert = [[MPMediaQuery alloc] init];
[quert addFilterPredicate: artistNamePredicate];
quert.groupingType = MPMediaGroupingArtist;</p>

<p>NSArray <em>itemsFromArtistQuery = [quert items];
NSArray </em>collectionsFromArtistQuery = [quert collections];
```
这一过程可以表示为（图来自<a href="https://developer.apple.com/library/ios/documentation/Audio/Conceptual/iPodLibraryAccess_Guide/AboutiPodLibraryAccess/AboutiPodLibraryAccess.html#//apple_ref/doc/uid/TP40008765-CH103-SW9">官方文档</a>）：</p>

<p><img src="/images/iOS-audio/mediaQuery.jpg" alt="" /></p>

<p>这里对于MediaQuery的用法就不再继续展开，关于这块内容并没有什么晦涩难懂的地方需要解释，大家可以通过阅读<a href="https://developer.apple.com/library/ios/documentation/Audio/Conceptual/iPodLibraryAccess_Guide/UsingTheiPodLibrary/UsingTheiPodLibrary.html#//apple_ref/doc/uid/TP40008765-CH101-SW1">官方文档</a>来详细了解其用法。</p>

<h3>MediaCollection</h3>

<p><code>MPMediaCollection</code>是MediaItem的合集，可以通过访问它的items属性来访问所有的MediaItem。</p>

<p><code>MPMediaPlaylist</code>是一个特殊的<code>MPMediaCollection</code>代表用户创建的播放列表，它会比MediaCollection包含更多的信息，比如播放列表的名字等等。这些属性可以通过<code>MPMediaEntity</code>的方法访问（MPMediaCollection是MPMediaEntity的子类，MPMediaItem也是）。
```objc
// Returns the value for the given entity property.
// MPMediaItem and MPMediaPlaylist have their own properties
&ndash; (id)valueForProperty:(NSString *)property;</p>

<p>// Executes a provided block with the fetched values for the given item properties, or nil if no value is available for a property.
// In some cases, enumerating the values for multiple properties can be more efficient than fetching each individual property with -valueForProperty:.
&ndash; (void)enumerateValuesForProperties:(NSSet <em>)properties usingBlock:(void (^)(NSString </em>property, id value, BOOL *stop))block NS_AVAILABLE_IOS(4_0);
```</p>

<h3>MediaItem</h3>

<p>通过MediaPicker和MediaQuery最终都会得到<code>MPMediaItem</code>，这个item中包含了许多信息。这些信息都可以通过<code>MPMediaEntity</code>的方法访问，其中参数非常多就不列举了具体可以参照MPMediaItem.h。</p>

<hr />

<h1>使用MPMusicPlayerController</h1>

<p>拿到iPod Library中的歌曲后就可以开始播放了。播放的方式有很多种，先介绍一下<code>MediaPlayer framework</code>中的<code>MPMusicPlayerController</code>类。</p>

<p>通过<code>MPMusicPlayerController</code>的类方法可以生成两种播放器，生成方法如下：</p>

<p>```objc
// Playing media items with the applicationMusicPlayer will restore the user&rsquo;s iPod state after the application quits.
+ (MPMusicPlayerController *)applicationMusicPlayer;</p>

<p>// Playing media items with the iPodMusicPlayer will replace the user&rsquo;s current iPod state.
+ (MPMusicPlayerController *)iPodMusicPlayer;
```</p>

<p>这两个方法看似生成了一样的对象，但它们的行为却有很大不同。从Apple写的注释上我们可以很清楚的发现它们的区别。<code>+applicationMusicPlayer</code>不会继承来自iOS系统自带的iPod应用中的播放状态，同时也不会覆盖iPod的播放状态。而<code>+iPodMusicPlayer</code>完全继承iPod应用的播放状态（甚至是播放时间），对其实例的任何操作也会覆盖到iPod应用。对<code>+iPodMusicPlayer</code>方法command+点击后可以看到更详细的注释。</p>

<p>```</p>

<pre><code>The iPod music player employs the iPod app on your behalf. On instantiation, it takes on the current iPod app state and controls that state as your app runs. Specifically, the shared state includes the following:
Repeat mode (see “Repeat Modes”)
Shuffle mode (see “Shuffle Modes”
Now-playing item (see nowPlayingItem)
Playback state (see playbackState)

Other aspects of iPod state, such as the on-the-go playlist, are not shared. Music that is playing continues to play when your app moves to the background.
</code></pre>

<p>```
说白了，当在使用iPodMusicPlayerv其实并不是你的程序在播放音频，而是你的程序在操纵iPod应用播放音频，即使你的程序crash了或者被kill了，音乐也不会因此停止。</p>

<p>而对于<code>+applicationMusicPlayer</code>通过command+点击可以看到：</p>

<p>```
The application music player plays music locally within your app. It does not affect the iPod state.
When your app moves to the background, the music player stops if it was playing.</p>

<p>```
从注释中可以知道这个方法返回的对象虽然不是调用iPod应用播放的也不会影响到iPod应用，但它有个很大的缺点：无法后台播放，即使你在active了audioSession并且在app的设置中设置了Background Audio同样不会奏效。</p>

<p>综上所述，一般在开发音乐软件时很少用到这两个接口来进行iPod Library的播放，大部分开发者都是用这个类中的volme来调整系统音量的（这个属性在SDK 7中也被deprecate掉了）。如果你想用到这个类进行播放的话，这里需要提个醒，给<code>MPMusicPlayerController</code>设置需要播放的音乐时要使用下面两个方法：</p>

<p><code>objc
// Call -play to begin playback after setting an item queue source. Setting a query will implicitly use MPMediaGroupingTitle.
- (void)setQueueWithQuery:(MPMediaQuery *)query;
- (void)setQueueWithItemCollection:(MPMediaItemCollection *)itemCollection;
</code>
而不是这个属性：</p>

<p><code>objc
// Returns the currently playing media item, or nil if none is playing.
// Setting the nowPlayingItem to an item in the current queue will begin playback at that item.
@property(nonatomic, copy) MPMediaItem *nowPlayingItem;
</code>
光看名字很容易被<code>nowPlayingItem</code>这个属性迷惑，它的意思其实是说在设置了MediaQuery或者MediaCollection之后再设置这个nowPlayingItem可以让播放器从这个item开始播放，前提是这个item需要在MediaQuery或者MediaCollection的.items集合内。</p>

<hr />

<h1>使用AVAudioPlayer和AVPlayer</h1>

<p>除了使用MediaPlayer中的类还有很多其他方法来进行iPod播放，其中做的比较出色的是<code>AVFoundation</code>中的<code>AVAudioPlayer</code>和<code>AVPlayer</code>。</p>

<p>这两个类的都有通过NSURL生成实例的初始化方法：</p>

<p>```objc
//AVAudioPlayer
&ndash; (id)initWithContentsOfURL:(NSURL *)url error:(NSError **)outError;</p>

<p>//AVPlayer
+ (id)playerWithURL:(NSURL <em>)URL;
&ndash; (id)initWithURL:(NSURL </em>)URL;
```</p>

<p>其中的NSURL正是来自于<code>MPMediaItem</code>的<code>MPMediaItemPropertyAssetURL</code>属性。</p>

<p><code>objc
//A URL pointing to the media item,
//from which an AVAsset object (or other URL-based AV Foundation object) can be created, with any options as desired.
//Value is an NSURL object.
MP_EXTERN NSString *const MPMediaItemPropertyAssetURL;
</code>
上面讲到<code>MPMediaItem</code>时已经提到了它是<code>MPMediaEntity</code>子类，可以通过<code>-valueForProperty:</code>方法访问其中的属性。通过传入<code>MPMediaItemPropertyAssetURL</code>就可以得到当前MediaItem对应的URL（ipod-library://xxxxx），生成Player进行播放。大致代码如下：</p>

<p>```objc
@interface MyClass : NSObject
{</p>

<pre><code>AVAudioPlayer *_player;
//AVPlayer *_player;
</code></pre>

<p>}</p>

<p>//设置AudioSession
[[AVAudioSession sharedInstance] setActive:YES error:nil];
[[AVAudioSession sharedInstance] setCategory:AVAudioSessionCategoryPlayback error:nil];</p>

<p>//play
NSError <em>error = nil;
MPMediaItem </em>item = &hellip;;
NSURL *url = [item valueForProperty:MPMediaItemPropertyAssetURL];
<em>player = [[AVAudioPlayer alloc] initWithContentsOfURL:url error:&amp;error];
//</em>player = [AVPlayer playerWithURL:url];
[_player play];</p>

<p>```</p>

<p><strong>注意：这里我需要更正一下，之前我在<a href="/blog/2014/07/08/audio-in-ios-2/">第二篇</a>讲到AudioSession时写了这样一段话<code>在使用AVAudioPlayer/AVPlayer时可以不用关心AudioSession的相关问题，Apple已经把AudioSession的处理过程封装了...</code>。这段话不对，我把AVFoundation和Mediaplayer混淆了，在写的时候也没注意，应该是在使用MPMusicPlayerController播放时不需要关心AudioSession的问题。</strong></p>

<hr />

<h1>读取和导出数据</h1>

<p>前面说到使用<code>MPMediaItem</code>的<code>MPMediaItemPropertyAssetURL</code>属性可以得到一个表示当前MediaItem的NSURL，有了这个NSURL我们使用AVFoundation中的类进行播放。播放只是最基本的需求，有了这个URL我们可以做更多更有趣的事情。</p>

<p>在AVFoundation中还有两个有趣的类：<code>AVAssetReader</code>和<code>AVAssetExportSession</code>。它们可以把iPod Library中的指定歌曲以指定的音频格式导出到内存中或者硬盘中，这个指定的格式包括PCM。这是一个激动人心的特性，有了PCM数据我们就可以做很多很多其他的事情了。</p>

<p>这部分如果要展开的话还会有相当多的内容，国外的先辈们早在2010年就已经发掘了这两个类的用法，详细参见<a href="http://www.subfurther.com/blog/2010/07/19/from-iphone-media-library-to-pcm-samples-in-dozens-of-confounding-potentially-lossy-steps/">这里</a>和<a href="http://www.subfurther.com/blog/2010/12/13/from-ipod-library-to-pcm-samples-in-far-fewer-steps-than-were-previously-necessary/">这里</a>。这两篇讲的比较详细并且附有Sample（其中还涉及了一些Extended Audio File Services的内容），如果里面Sample无法下载可以从点击<a href="/files/MediaLibraryExportThrowaway1.zip">MediaLibraryExportThrowaway1.zip</a>和<a href="/files/VTM_AViPodReader.zip">VTM_AViPodReader.zip</a>下载。</p>

<p><strong>需要注意的是在使用<code>AVAssetReader</code>的过程中如果访问系统的相机或者照片可能会使<code>AVAssetReader</code>产生<code>AVErrorOperationInterrupted</code>错误，此时需要重新生成Reader后调用<code>-startReading</code>才可以继续读取数据。</strong></p>

<hr />

<h1>小结</h1>

<p>本篇介绍了一些与iPod Library相关的内容，小结一下：</p>

<ul>
<li><p>Apple提供两种方法来访问iPod Library，它们分别是<code>MPMediaPickerController</code>和<code>MPMediaQuery</code>；</p></li>
<li><p><code>MPMediaPickerController</code>和<code>MPMediaQuery</code>最后输出给开发者的对象是<code>MPMediaItem</code>，<code>MPMediaItem</code>的属性需要通过<code>-valueForProperty:</code>方法获取了；</p></li>
<li><p><code>MPMusicPlayerController</code>可以用来播放<code>MPMediaItem</code>，但有很多局限性，使用时需要根据不同的使用场景来决定用哪个类方法生成实例；</p></li>
<li><p><code>AVAudioPlayer</code>和<code>AVPlayer</code>也可以用来播放<code>MPMediaItem</code>，这两个类的功能比较完善，推荐使用，在使用之前别忘记设置AudioSession；</p></li>
<li><p><code>MPMediaItem</code>可以得到对应的URL，这个URL可以用来做很多事情，例如用<code>AVAssetReader</code>和<code>AVAssetExportSession</code>可以导出其中的数据；</p></li>
</ul>


<hr />

<h1>下篇预告</h1>

<p>下一篇会讲一些关于NowPlayingCenter和RemoteControl的内容（就是在锁屏界面和ControlCenter中显示的歌曲信息以及上面的那些播放控制按钮）。</p>

<hr />

<h1>参考资料</h1>

<p><a href="https://developer.apple.com/library/ios/documentation/Audio/Conceptual/iPodLibraryAccess_Guide/Introduction/Introduction.html#//apple_ref/doc/uid/TP40008765">iPod Library Access Programming Guide</a></p>

<p><a href="https://developer.apple.com/library/ios/documentation/Audio/Conceptual/iPodLibraryAccess_Guide/AboutiPodLibraryAccess/AboutiPodLibraryAccess.html#//apple_ref/doc/uid/TP40008765-CH103-SW9">About iPod Library Access</a></p>

<p><a href="https://developer.apple.com/library/ios/documentation/Audio/Conceptual/iPodLibraryAccess_Guide/UsingtheMediaItemPicker/UsingtheMediaItemPicker.html#//apple_ref/doc/uid/TP40008765-CH104-SW1">Using the Media Item Picker</a></p>

<p><a href="https://developer.apple.com/library/ios/documentation/Audio/Conceptual/iPodLibraryAccess_Guide/UsingTheiPodLibrary/UsingTheiPodLibrary.html#//apple_ref/doc/uid/TP40008765-CH101-SW1">Using the iPod Library</a></p>

<p><a href="https://developer.apple.com/library/ios/documentation/Audio/Conceptual/iPodLibraryAccess_Guide/UsingMediaPlayback/UsingMediaPlayback.html#//apple_ref/doc/uid/TP40008765-CH100-SW1">Using Media Playback</a></p>

<p><a href="http://www.subfurther.com/blog/2010/07/19/from-iphone-media-library-to-pcm-samples-in-dozens-of-confounding-potentially-lossy-steps/">From iPhone Media Library to PCM Samples in Dozens of Confounding, Potentially Lossy Steps</a></p>

<p><a href="http://www.subfurther.com/blog/2010/12/13/from-ipod-library-to-pcm-samples-in-far-fewer-steps-than-were-previously-necessary/">From iPod Library to PCM Samples in Far Fewer Steps Than Were Previously Necessary</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iOS音频播放 (六)：简单的音频播放器实现]]></title>
    <link href="http://msching.github.io/blog/2014/08/09/audio-in-ios-6/"/>
    <updated>2014-08-09T15:55:22+08:00</updated>
    <id>http://msching.github.io/blog/2014/08/09/audio-in-ios-6</id>
    <content type="html"><![CDATA[<p>在前几篇中我分别讲到了<code>AudioSession</code>、<code>AudioFileStream</code>、<code>AudioFile</code>、<code>AudioQueue</code>，这些类的功能已经涵盖了<a href="/blog/2014/07/07/audio-in-ios/">第一篇</a>中所提到的音频播放所需要的步骤：</p>

<ol>
<li>读取MP3文件   <code>NSFileHandle</code></li>
<li>解析采样率、码率、时长等信息，分离MP3中的音频帧  <code>AudioFileStream</code>/<code>AudioFile</code></li>
<li>对分离出来的音频帧解码得到PCM数据  <code>AudioQueue</code></li>
<li><del>对PCM数据进行音效处理（均衡器、混响器等，非必须）</del>  <code>省略</code></li>
<li>把PCM数据解码成音频信号  <code>AudioQueue</code></li>
<li>把音频信号交给硬件播放    <code>AudioQueue</code></li>
<li>重复1-6步直到播放完成</li>
</ol>


<p>下面我们就讲讲述如何用这些部件组成一个简单的<code>本地音乐播放器</code>，这里我会用到<strong>AudioSession</strong>、<strong>AudioFileStream</strong>、<strong>AudioFile</strong>、<strong>AudioQueue</strong>。</p>

<p><strong>注意：在阅读本篇请实现阅读并理解前面1-5篇的内容以及2-5篇最后给出的封装类，本篇中的播放器实现将基于前面几篇中给出的<a href="https://github.com/msching/MCAudioSession">MCAudioSession</a>、<a href="https://github.com/msching/MCAudioFileStream">MCAudioFileStream</a>、<a href="https://github.com/msching/MCAudioFile">MCAudioFile</a>和<a href="https://github.com/msching/MCSimpleAudioPlayer/blob/master/MCSimpleAudioPlayerDemo/MCSimpleAudioPlayer/MCAudioOutputQueue.h">MCAudioOutputQueue</a>进行实现。</strong></p>

<!--more-->


<hr />

<h1>AudioFileStream vs AudioFile</h1>

<p>解释一下为什么我要同时使用<strong>AudioFileStream</strong>和<strong>AudioFile</strong>。</p>

<p>第一，<code>对于网络流播必须有AudioFileStream的支持</code>，这是因为我们在<a href="/blog/2014/07/19/audio-in-ios-4/">第四篇</a>中提到过<strong>AudioFile</strong>在Open时会要求使用者提供数据，如果提供的数据不足会直接跳过并且返回错误码，而数据不足的情况在网络流中很常见，故无法使用<strong>AudioFile</strong>单独进行网络流数据的解析；</p>

<p>第二，<code>对于本地音乐播放选用AudioFile更为合适</code>，原因如下：</p>

<ol>
<li><strong>AudioFileStream</strong>的主要是用在流播放中虽然不限于网络流和本地流，但流数据是按顺序提供的所以<strong>AudioFileStream</strong>也是顺序解析的，被解析的音频文件还是需要符合流播放的特性，对于不符合的本地文件<strong>AudioFileStream</strong>会在Parse时返回<code>NotOptimized</code>错误；</li>
<li><strong>AudioFile</strong>的解析过程并不是顺序的，它会在解析时通过回调向使用者索要某个位置的数据，即使数据在文件末尾也不要紧，所以<strong>AudioFile</strong>适用于所有类型的音频文件；</li>
</ol>


<p>基于以上两点我们可以得出这样一个结论：<code>一款完整功能的播放器应当同时使用AudioFileStream和AudioFile</code>，用<strong>AudioFileStream</strong>来应对可以进行流播放的音频数据，以达到边播放边缓冲的最佳体验，用<strong>AudioFile</strong>来处理无法流播放的音频数据，让用户在下载完成之后仍然能够进行播放。</p>

<p>本来这个Demo应该做成基于网络流的音频播放，但由于最近比较忙一直过着公司和床两点一线的生活，来不及写网络流和文件缓存的模块，所以就用本地文件代替了，所以最终在Demo会先尝试用<strong>AudioFileStream</strong>解析数据，如果失败再尝试使用<strong>AudioFile</strong>以达到模拟网络流播放的效果。</p>

<hr />

<h1>准备工作</h1>

<p>第一件事当然是要创建一个新工程，这里我选择了的模板是SingleView，工程名我把它命名为<code>MCSimpleAudioPlayerDemo</code>：</p>

<p><img src="/images/iOS-audio/createproject.jpg" alt="" /></p>

<p>创建完工程之后去到Target属性的<code>Capabilities</code>选项卡设置<code>Background Modes</code>，把<code>Audio and Airplay</code>勾选，这样我们的App就可以在进入后台之后继续播放音乐了：</p>

<p><img src="/images/iOS-audio/setBackgroundPlayback.jpg" alt="" /></p>

<p>接下来我们需要搭建一个简单的UI，在storyboard上创建两个UIButton和一个UISlider，Button用来做播放器的播放、暂停、停止等功能控制，Slider用来显示播放进度和seek。把这些UI组件和ViewController的属性/方法关联上之后简单的UI也就完成了。</p>

<p><img src="/images/iOS-audio/simpleUI.jpg" alt="" /></p>

<hr />

<h1>接口定义</h1>

<p>下面来创建播放器类<code>MCSimpleAudioPlayer</code>，首先是初始化方法（感谢<a href="http://weibo.com/onevcat?topnav=1&amp;wvr=5&amp;topsug=1">@喵神</a>的<a href="https://github.com/onevcat/VVDocumenter-Xcode">VVDocumenter</a>）：</p>

<p><code>objc
/**
 *  初始化方法
 *
 *  @param filePath 文件绝对路径
 *  @param fileType 文件类型，作为后续创建AudioFileStream和AudioQueue的Hint使用
 *
 *  @return player对象
 */
- (instancetype)initWithFilePath:(NSString *)filePath fileType:(AudioFileTypeID)fileType;
</code>
另外播放器作为一个典型的状态机，各种状态也是必不可少的，这里我只简单的定义了四种状态：</p>

<p>```objc
typedef NS_ENUM(NSUInteger, MCSAPStatus)
{</p>

<pre><code>MCSAPStatusStopped = 0,
MCSAPStatusPlaying = 1,
MCSAPStatusWaiting = 2,
MCSAPStatusPaused = 3,
</code></pre>

<p>};
```</p>

<p>再加上一些必不可少的属性和方法组成了<code>MCSimpleAudioPlayer.h</code></p>

<p>```objc
@interface MCSimpleAudioPlayer : NSObject</p>

<p>@property (nonatomic,copy,readonly) NSString *filePath;
@property (nonatomic,assign,readonly) AudioFileTypeID fileType;</p>

<p>@property (nonatomic,readonly) MCSAPStatus status;
@property (nonatomic,readonly) BOOL isPlayingOrWaiting;
@property (nonatomic,assign,readonly) BOOL failed;</p>

<p>@property (nonatomic,assign) NSTimeInterval progress;
@property (nonatomic,readonly) NSTimeInterval duration;</p>

<ul>
<li><p>(instancetype)initWithFilePath:(NSString *)filePath fileType:(AudioFileTypeID)fileType;</p></li>
<li><p>(void)play;</p></li>
<li>(void)pause;</li>
<li>(void)stop;
@end
```</li>
</ul>


<hr />

<h1>初始化</h1>

<p>在init方法中创建一个NSFileHandle的实例以用来读取数据并交给AudioFileStream解析，另外也可以根据生成的实例是否是nil来判断是否能够读取文件，如果返回的是nil就说明文件不存在或者没有权限那么播放也就无从谈起了。</p>

<p><code>objc
_fileHandler = [NSFileHandle fileHandleForReadingAtPath:_filePath];
</code></p>

<p>通过NSFileManager获取文件大小</p>

<p><code>objc
_fileSize = [[[NSFileManager defaultManager] attributesOfItemAtPath:_filePath error:nil] fileSize];
</code>
初始化方法到这里就结束了，作为一个播放器我们自然不能在主线程进行播放，我们需要创建自己的播放线程。</p>

<p>创建一个成员变量<code>_started</code>来表示播放流程是否已经开始，在<code>-play</code>方法中如果<code>_started</code>为NO就创建线程<code>_thread</code>并以<code>-threadMain</code>方法作为main，否则说明线程已经创建并且在播放流程中：</p>

<p>```
&ndash; (void)play
{</p>

<pre><code>if (!_started)
{
    _started = YES;
    _thread = [[NSThread alloc] initWithTarget:self selector:@selector(threadMain) object:nil];
    [_thread start];
}
else
{
    //如果是Pause状态就resume
}
</code></pre>

<p>}
<code>``
接下来就可以在</code>-threadMain`进行音频播放相关的操作了。</p>

<hr />

<h1>创建AudioSession</h1>

<p>iOS音频播放的第一步，自然是要创建<code>AudioSession</code>，这里引入<a href="/blog/2014/07/08/audio-in-ios-2/">第二篇</a>末尾给出的AudioSession封装<a href="https://github.com/msching/MCAudioSession">MCAudioSession</a>，当然各位也可以使用<code>AVAudioSession</code>。</p>

<p>初始化的工作会在调用单例方法时进行，下一步是设置Category。</p>

<p>```objc
//初始化并且设置Category
[[MCAudioSession sharedInstance] setCategory:kAudioSessionCategory_MediaPlayback error:NULL];</p>

<p>```
成功之后启用AudioSession，还有别忘了监听Interrupt通知。</p>

<p>```objc
if ([[MCAudioSession sharedInstance] setCategory:kAudioSessionCategory_MediaPlayback error:NULL])
{</p>

<pre><code>//active audiosession
[[NSNotificationCenter defaultCenter] addObserver:self selector:@selector(interruptHandler:) name:MCAudioSessionInterruptionNotification object:nil];
if ([[MCAudioSession sharedInstance] setActive:YES error:NULL])
{
    //go on
}
</code></pre>

<p>}
```</p>

<hr />

<h1>读取、解析音频数据</h1>

<p>成功创建并启用AudioSession之后就可以进入播放流程了，播放是一个无限循环的过程，所以我们需要一个while循环，在文件没有被播放完成之前需要反复的读取、解析、播放。那么第一步是需要读取并解析数据。按照之前说的我们会先使用<code>AudioFileStream</code>，引入<a href="/blog/2014/07/09/audio-in-ios-3/">第三篇</a>末尾给出的AudioFileStream封装<a href="https://github.com/msching/MCAudioFileStream">MCAudioFileStream</a>。</p>

<p>创建AudioFileStream，<strong>MCAudioFileStream</strong>的init方法会完成这项工作，如果创建成功就设置delegate作为Parse数据的回调。</p>

<p>```objc
<em>audioFileStream = [[MCAudioFileStream alloc] initWithFileType:</em>fileType fileSize:_fileSize error:&amp;error];
if (!error)
{</p>

<pre><code>_audioFileStream.delegate = self;
</code></pre>

<p>}
```</p>

<p>接下来要读取数据并且解析，用成员变量<code>_offset</code>表示<code>_fileHandler</code>已经读取文件位置，其主要作用是来判断Eof。调用<strong>MCAudioFileStream</strong>的<code>-parseData:error:</code>方法来对数据进行解析。</p>

<p>```objc
NSData *data = [<em>fileHandler readDataOfLength:1000];
</em>offset += [data length];
if (<em>offset >= </em>fileSize)
{</p>

<pre><code>isEof = YES;
</code></pre>

<p>}
[_audioFileStream parseData:data error:&amp;error];
if (error)
{</p>

<pre><code>//解析失败，换用AudioFile
</code></pre>

<p>}</p>

<p><code>``
解析完文件头之后**MCAudioFileStream**的</code>readyToProducePackets<code>属性会被置为YES，此后所有的Parse方法都回触发</code>-audioFileStream:audioDataParsed:<code>方法并传递</code>MCParsedAudioData`的数组来保存解析完成的数据。这样就需要一个buffer来存储这些解析完成的音频数据。</p>

<p>于是创建了<code>MCAudioBuffer</code>类来管理所有解析完成的数据：</p>

<p>```objc
@interface MCAudioBuffer : NSObject</p>

<ul>
<li><p>(instancetype)buffer;</p></li>
<li><p>(void)enqueueData:(MCParsedAudioData *)data;</p></li>
<li><p>(void)enqueueFromDataArray:(NSArray *)dataArray;</p></li>
<li><p>(BOOL)hasData;</p></li>
<li><p>(UInt32)bufferedSize;</p></li>
<li><p>(NSData <em>)dequeueDataWithSize:(UInt32)requestSize
                    packetCount:(UInt32 </em>)packetCount
                   descriptions:(AudioStreamPacketDescription **)descriptions;</p></li>
<li><p>(void)clean;
@end</p></li>
</ul>


<p>```</p>

<p>创建一个<strong>MCAudioBuffer</strong>的实例<code>_buffer</code>，解析完成的数据都会通过<code>enqueue</code>方法存储到<strong>_buffer</strong>中，在需要的使用可以通过<code>dequeue</code>取出来使用。</p>

<p>```objc
_buffer = [MCAudioBuffer buffer]; //初始化方法中创建</p>

<p>//AudioFileStream解析完成的数据都被存储到了_buffer中
&ndash; (void)audioFileStream:(MCAudioFileStream <em>)audioFileStream audioDataParsed:(NSArray </em>)audioData
{</p>

<pre><code>[_buffer enqueueFromDataArray:audioData];
</code></pre>

<p>}
```</p>

<p>如果遇到<strong>AudioFileStream</strong>解析失败的话，转而使用<strong>AudioFile</strong>，引入<a href="/blog/2014/07/19/audio-in-ios-4/">第四篇</a>末尾给出的AudioFile封装<a href="https://github.com/msching/MCAudioFile">MCAudioFile</a>（之前没有给出，最近补上的）。</p>

<p>```objc
_audioFileStream parseData:data error:&amp;error];
if (error)
{</p>

<pre><code>//解析失败，换用AudioFile
_usingAudioFile = YES;
continue;
</code></pre>

<p>}
```</p>

<p>```objc
if (_usingAudioFile)
{</p>

<pre><code>if (!_audioFile)
{
    _audioFile = [[MCAudioFile alloc] initWithFilePath:_filePath fileType:_fileType];
}
if ([_buffer bufferedSize] &lt; _bufferSize || !_audioQueue)
{    
    //AudioFile解析完成的数据都被存储到了_buffer中
    NSArray *parsedData = [_audioFile parseData:&amp;isEof];
    [_buffer enqueueFromDataArray:parsedData];
}
</code></pre>

<p>}</p>

<p>```</p>

<p>使用<strong>AudioFile</strong>时同样需要<strong>NSFileHandle</strong>来读取文件数据，但由于其回获取数据的特性我把FileHandle的相关操作都封装进去了，所以使用<strong>MCAudioFile</strong>解析数据时直接调用Parse方法即可。</p>

<hr />

<h1>播放</h1>

<p>有了解析完成的数据，接下来就该AudioQueue出场了，引入<a href="/blog/2014/08/02/audio-in-ios-5/">第五篇</a>末尾提到的AudioQueue的封装<a href="https://github.com/msching/MCSimpleAudioPlayer/blob/master/MCSimpleAudioPlayerDemo/MCSimpleAudioPlayer/MCAudioOutputQueue.h">MCAudioOutputQueue</a>。</p>

<p>首先创建AudioQueue，由于AudioQueue需要实现创建重用buffer所以需要事先确定bufferSize，这里我设置的bufferSize是近似0.1秒的数据量，计算bufferSize需要用到的duration和audioDataByteCount可以从<strong>MCAudioFileStream</strong>或者<strong>MCAudioFile</strong>中获取。有了bufferSize之后，加上数据格式format参数和magicCookie（部分音频格式需要）就可以生成AudioQueue了。</p>

<p>```objc
&ndash; (BOOL)createAudioQueue
{</p>

<pre><code>if (_audioQueue)
{
    return YES;
}

NSTimeInterval duration = _usingAudioFile ? _audioFile.duration : _audioFileStream.duration;
UInt64 audioDataByteCount = _usingAudioFile ? _audioFile.audioDataByteCount : _audioFileStream.audioDataByteCount;
_bufferSize = 0;
if (duration != 0)
{
    _bufferSize = (0.1 / duration) * audioDataByteCount;
}

if (_bufferSize &gt; 0)
{
    AudioStreamBasicDescription format = _usingAudioFile ? _audioFile.format : _audioFileStream.format;
    NSData *magicCookie = _usingAudioFile ? [_audioFile fetchMagicCookie] : [_audioFileStream fetchMagicCookie];
    _audioQueue = [[MCAudioOutputQueue alloc] initWithFormat:format bufferSize:_bufferSize macgicCookie:magicCookie];
    if (!_audioQueue.available)
    {
        _audioQueue = nil;
        return NO;
    }
}
return YES;
</code></pre>

<p>}
<code>
接下来从**_buffer**中读出解析完成的数据，交给AudioQueue播放。如果全部播放完毕了就调用一下`-flush`让AudioQueue把剩余数据播放完毕。这里需要注意的是**MCAudioOutputQueue**的`-playData`方法在调用时如果没有可以重用的buffer的话会阻塞当前线程直到`AudioQueue`回调方法送出可重用的buffer为止。
</code>
UInt32 packetCount;
AudioStreamPacketDescription <em>desces = NULL;
NSData </em>data = [<em>buffer dequeueDataWithSize:</em>bufferSize packetCount:&amp;packetCount descriptions:&amp;desces];
if (packetCount != 0)
{</p>

<pre><code>[_audioQueue playData:data packetCount:packetCount packetDescriptions:desces isEof:isEof];
free(desces);

if (![_buffer hasData] &amp;&amp; isEof)
{
    [_audioQueue flush];
    break;
}
</code></pre>

<p>}
```</p>

<hr />

<h1>暂停 &amp; 恢复</h1>

<p>暂停方法很简单，调用<strong>MCAudioOutputQueue</strong>的<code>-pause</code>方法就可以了，但要注意的是需要和<code>-playData:</code>同步调用，否则可能引起一些问题（比如触发了pause实际由于并发操作没有真正pause住）。</p>

<p>同步的方法可以采用加锁的方式，也可以通过标志位在<code>threadMain</code>中进行Pause，Demo中我使用了后者。</p>

<p>```objc
//pause方法
&ndash; (void)pause
{</p>

<pre><code>if (self.isPlayingOrWaiting)
{
    _pauseRequired = YES;
}
</code></pre>

<p>}</p>

<p>//threadMain中
&ndash; (void)threadMain
{</p>

<pre><code>...

//pause
if (_pauseRequired)
{
    [self setStatusInternal:MCSAPStatusPaused];
    [_audioQueue pause];
    [self _mutexWait];
    _pauseRequired = NO;
}

//play
...
</code></pre>

<p>}
```
在暂停后还要记得阻塞线程。</p>

<p>恢复只要调用<strong>AudioQueue</strong> start方法就可以了，同时记得signal让线程继续跑</p>

<p>```objc
&ndash; (void)_resume
{</p>

<pre><code>//AudioQueue的start方法被封装到了MCAudioOutputQueue的resume方法中
[_audioQueue resume];
[self _mutexSignal];
</code></pre>

<p>}
```</p>

<hr />

<h1>播放进度 &amp; Seek</h1>

<p>对于播放进度我在<a href="/blog/2014/08/02/audio-in-ios-5/">第五篇</a>讲<strong>AudioQueue</strong>时已经提到过了，使用<code>AudioQueueGetCurrentTime</code>方法可以获取<code>实际播放的时间</code>如果Seek之后需要根据计算timingOffset，然后根据timeOffset来计算最终的播放进度：</p>

<p>```objc
&ndash; (NSTimeInterval)progress
{</p>

<pre><code>return _timingOffset + _audioQueue.playedTime;
</code></pre>

<p>}
```
timingOffset的计算在Seek进行，Seek操作和暂停操作一样需要和其他<strong>AudioQueue</strong>的操作同步进行，否则可能造成一些并发问题。</p>

<p>```objc
//seek方法
&ndash; (void)setProgress:(NSTimeInterval)progress
{</p>

<pre><code>_seekRequired = YES;
_seekTime = progress;
</code></pre>

<p>}
```
在seek时为了防止播放进度跳动，修改一下获取播放进度的方法：</p>

<p>```objc
&ndash; (NSTimeInterval)progress
{</p>

<pre><code>if (_seekRequired)
{
    return _seekTime;
}
return _timingOffset + _audioQueue.playedTime;
</code></pre>

<p>}
<code>``
下面是</code>threadMain`中的Seek操作</p>

<p>```objc
if (_seekRequired)
{</p>

<pre><code>[self setStatusInternal:MCSAPStatusWaiting];

_timingOffset = _seekTime - _audioQueue.playedTime;
[_buffer clean];
if (_usingAudioFile)
{
    [_audioFile seekToTime:_seekTime];
}
else
{
    _offset = [_audioFileStream seekToTime:&amp;_seekTime];
    [_fileHandler seekToFileOffset:_offset];
}
_seekRequired = NO;
[_audioQueue reset];
</code></pre>

<p>}
```
Seek时需要做如下事情：</p>

<ol>
<li>计算timingOffset</li>
<li>清除之前残余在<code>_buffer</code>中的数据</li>
<li>挪动NSFileHandle的游标</li>
<li>清除<strong>AudioQueue</strong>中已经Enqueue的数据</li>
<li>如果有用到音效器的还需要清除音效器里的残余数据</li>
</ol>


<hr />

<h1>打断</h1>

<p>在接到Interrupt通知时需要处理打断，下面是打断的处理方法：</p>

<p>```objc
&ndash; (void)interruptHandler:(NSNotification *)notification
{</p>

<pre><code>UInt32 interruptionState = [notification.userInfo[MCAudioSessionInterruptionStateKey] unsignedIntValue];

if (interruptionState == kAudioSessionBeginInterruption)
{
    _pausedByInterrupt = YES;
    [_audioQueue pause];
    [self setStatusInternal:MCSAPStatusPaused];

}
else if (interruptionState == kAudioSessionEndInterruption)
{
    AudioSessionInterruptionType interruptionType = [notification.userInfo[MCAudioSessionInterruptionTypeKey] unsignedIntValue];
    if (interruptionType == kAudioSessionInterruptionType_ShouldResume)
    {
        if (self.status == MCSAPStatusPaused &amp;&amp; _pausedByInterrupt)
        {
            if ([[MCAudioSession sharedInstance] setActive:YES error:NULL])
            {
                [self play];
            }
        }
    }
}
</code></pre>

<p>}
```</p>

<p>这里需要注意，打断操作我放在了主线程进行而并非放到新开的线程中进行，原因如下：</p>

<ul>
<li><p>一旦打断开始<strong>AudioSession</strong>被抢占后音频立即被打断，此时<strong>AudioQueue</strong>的所有操作会暂停，这就意味着不会有任何数据消耗回调产生；</p></li>
<li><p>我这个Demo的线程模型中在向<strong>AudioQueue</strong> Enqueue了足够多的数据之后会阻塞当前线程等待数据消耗的回调才会signal让线程继续跑；</p></li>
</ul>


<p>于是就得到了这样的结论：一旦打断开始我创建的线程就会被阻塞，所以我需要在主线程来处理暂停和恢复播放。</p>

<hr />

<h1>停止 &amp; 清理</h1>

<p>停止操作也和其他操作一样会放到<code>threadMain</code>中执行</p>

<p>```objc
&ndash; (void)stop
{</p>

<pre><code>_stopRequired = YES;
[self _mutexSignal];
</code></pre>

<p>}</p>

<p>//treadMain中
if (_stopRequired)
{</p>

<pre><code>_stopRequired = NO;
_started = NO;
[_audioQueue stop:YES];
break;
</code></pre>

<p>}
```</p>

<p>在播放被停止或者出错时会进入到清理流程，这里需要做一大堆操作，清理各种数据，关闭AudioSession，清除各种标记等等。</p>

<p>```objc
&ndash; (void)cleanup
{</p>

<pre><code>//reset file
_offset = 0;
[_fileHandler seekToFileOffset:0];

//deactive audiosession
[[MCAudioSession sharedInstance] setActive:NO error:NULL];
[[NSNotificationCenter defaultCenter] removeObserver:self name:MCAudioSessionInterruptionNotification object:nil];

//clean buffer
[_buffer clean];

_usingAudioFile = NO;
//close audioFileStream
[_audioFileStream close];

//close audiofile
[_audioFile close];

//stop audioQueue
[_audioQueue stop:YES];

//destory mutex &amp; cond
[self _mutexDestory];

_started = NO;
_timingOffset = 0;
_seekTime = 0;
_seekRequired = NO;
_pauseRequired = NO;
_stopRequired = NO;

//reset status
[self setStatusInternal:MCSAPStatusStopped];
</code></pre>

<p>}</p>

<h2>```</h2>

<h1>连接播放器UI</h1>

<p>播放器代码完成后就需要和UI连起来让播放器跑起来了。</p>

<p>在viewDidLoad时创建一个播放器：</p>

<p>```objc</p>

<ul>
<li><p>(void)viewDidLoad
{
  [super viewDidLoad];</p>

<p>  if (!<em>player)
  {
      NSString *path = [[NSBundle mainBundle] pathForResource:@&ldquo;MP3Sample&rdquo; ofType:@&ldquo;mp3&rdquo;];
      </em>player = [[MCSimpleAudioPlayer alloc] initWithFilePath:path fileType:kAudioFileMP3Type];</p>

<pre><code>  [_player addObserver:self forKeyPath:@"status" options:NSKeyValueObservingOptionNew context:nil];
</code></pre>

<p>  }
  [_player play];
}
```
对播放器的status属性KVO用来操作播放和暂停按钮的状态以及播放进度timer的开启和暂停：</p></li>
</ul>


<p>```objc
&ndash; (void)observeValueForKeyPath:(NSString <em>)keyPath ofObject:(id)object change:(NSDictionary </em>)change context:(void *)context
{</p>

<pre><code>if (object == _player)
{
    if ([keyPath isEqualToString:@"status"])
    {
        [self performSelectorOnMainThread:@selector(handleStatusChanged) withObject:nil waitUntilDone:NO];
    }
}
</code></pre>

<p>}</p>

<ul>
<li><p>(void)handleStatusChanged
{
  if (_player.isPlayingOrWaiting)
  {
      [self.playOrPauseButton setTitle:@&ldquo;Pause&rdquo; forState:UIControlStateNormal];
      [self startTimer];</p>

<p>  }
  else
  {
      [self.playOrPauseButton setTitle:@&ldquo;Play&rdquo; forState:UIControlStateNormal];
      [self stopTimer];
      [self progressMove:nil];
  }
}</p></li>
</ul>


<p>```</p>

<p>播放进度交给timer来刷新：</p>

<p>```
&ndash; (void)startTimer
{</p>

<pre><code>if (!_timer)
{
    _timer = [NSTimer scheduledTimerWithTimeInterval:1 target:self selector:@selector(progressMove:) userInfo:nil repeats:YES];
    [_timer fire];
}
</code></pre>

<p>}</p>

<ul>
<li><p>(void)stopTimer
{
  if (<em>timer)
  {
      [</em>timer invalidate];
      _timer = nil;
  }
}</p></li>
<li><p>(void)progressMove:(id)sender
{
  //在seek时不要刷新slider的thumb位置
  if (!self.progressSlider.tracking)
  {
      if (<em>player.duration != 0)
      {
          self.progressSlider.value = </em>player.progress / _player.duration;
      }
      else
      {
          self.progressSlider.value = 0;
      }
  }
}
```</p></li>
</ul>


<p>监听slider的两个TouchUp时间来进行seek操作：</p>

<p>```objc
&ndash; (IBAction)seek:(id)sender
{</p>

<pre><code>_player.progress = _player.duration * self.progressSlider.value;
</code></pre>

<p>}
```</p>

<p>添加两个按钮的TouchUpInside事件进行播放控制：</p>

<p>```objc
&ndash; (IBAction)playOrPause:(id)sender
{</p>

<pre><code>if (_player.isPlayingOrWaiting)
{
    [_player pause];
}
else
{
    [_player play];
}
</code></pre>

<p>}</p>

<ul>
<li>(IBAction)stop:(id)sender
{
  [_player stop];
}</li>
</ul>


<p>```</p>

<hr />

<h1>进阶的内容</h1>

<p>关于简单播放器的构建就讲这么多，以下是一些音频播放相关的进阶内容，由于我自己也没有摸透它们所以暂时就不做详细介绍了以免误人子弟-_-，各位有兴趣可以研究一下，如果有疑问或者有新发现欢迎大家留言或者在<a href="http://weibo.com/msching">微博</a>上和我交流共同提高~</p>

<ol>
<li><code>AudioConverter</code>可以实现音频数据的转换，在播放流程中它可以充当解码器的角色，可以把压缩的音频数据解码成为PCM数据；</li>
<li><code>AudioUnit</code>作为比<code>AudioQueue</code>更底层的音频播放类库，Apple赋予了它更强大的功能，除了一般的播放功能之外它还能使用iPhone自带的多种均衡器对音效进行调节；</li>
<li><code>AUGraph</code>为<code>AudioUnit</code>提供音效处理功能（这个其实我一点也没接触过0_0）</li>
</ol>


<hr />

<h1>示例代码</h1>

<p>上面所讲述的内容对应的工程已经在github上了（<a href="https://github.com/msching/MCSimpleAudioPlayer">MCSimpleAudioPlayer</a>），有任何问题可以给我发issue~</p>

<div class="github-card" data-github="msching/MCSimpleAudioPlayer" data-width="400" data-height="" data-theme="default"></div>


<script src="//cdn.jsdelivr.net/github-cards/latest/widget.js"></script>


<hr />

<h1>下篇预告</h1>

<p>下一篇会介绍一下如何播放iOS系统<code>iPod Library</code>中的歌曲（俗称iPod音乐或者本地音乐）</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iOS音频播放 (五)：AudioQueue]]></title>
    <link href="http://msching.github.io/blog/2014/08/02/audio-in-ios-5/"/>
    <updated>2014-08-02T14:21:13+08:00</updated>
    <id>http://msching.github.io/blog/2014/08/02/audio-in-ios-5</id>
    <content type="html"><![CDATA[<p>在<a href="/blog/2014/07/09/audio-in-ios-3/">第三篇</a>和<a href="/blog/2014/07/19/audio-in-ios-4/">第四篇</a>中介绍了如何用<code>AudioStreamFile</code>和<code>AudioFile</code>解析音频数据格式、分离音频帧。下一步终于可以使用分离出来的音频帧进行播放了，本片中将来讲一讲如何使用<code>AudioQueue</code>播放音频数据。</p>

<!--more-->


<hr />

<h1>AudioQueue介绍</h1>

<p><code>AudioQueue</code>是<code>AudioToolBox.framework</code>中的一员，在<a href="https://developer.apple.com/library/ios/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/Introduction/Introduction.html#//apple_ref/doc/uid/TP40005343">官方文档</a>中Apple这样描述<code>AudioQueue</code>的：</p>

<p><code>Audio Queue Services provides a straightforward, low overhead way to record and play audio in iOS and Mac OS X. It is the recommended technology to use for adding basic recording or playback features to your iOS or Mac OS X application.</code></p>

<p>在文档中Apple推荐开发者使用<code>AudioQueue</code>来实现app中的播放和录音功能。这里我们会针对播放功能进行介绍。</p>

<p>对于支持的数据格式，Apple这样说：</p>

<pre><code>Audio Queue Services lets you record and play audio in any of the following formats:

* Linear PCM.
* Any compressed format supported natively on the Apple platform you are developing for.
* Any other format for which a user has an installed codec.
</code></pre>

<p>它支持<code>PCM</code>数据、iOS/MacOSX平台支持的压缩格式（MP3、AAC等）、其他用户可以自行提供解码器的音频数据（对于这一条，我的理解就是把音频格式自行解码成PCM数据后再给AudioQueue播放  ）。</p>

<hr />

<h1>AudioQueue的工作模式</h1>

<p>在使用<code>AudioQueue</code>之前首先必须理解其工作模式，它之所以这么命名是因为在其内部有一套缓冲队列（Buffer Queue）的机制。在<code>AudioQueue</code>启动之后需要通过<code>AudioQueueAllocateBuffer</code>生成若干个<code>AudioQueueBufferRef</code>结构，这些Buffer将用来存储即将要播放的音频数据，并且这些Buffer是受生成他们的<code>AudioQueue</code>实例管理的，内存空间也已经被分配（按照Allocate方法的参数），当<code>AudioQueue</code>被Dispose时这些Buffer也会随之被销毁。</p>

<p>当有音频数据需要被播放时首先需要被memcpy到<code>AudioQueueBufferRef</code>的mAudioData中（mAudioData所指向的内存已经被分配，之前<code>AudioQueueAllocateBuffer</code>所做的工作），并给mAudioDataByteSize字段赋值传入的数据大小。完成之后需要调用<code>AudioQueueEnqueueBuffer</code>把存有音频数据的Buffer插入到<code>AudioQueue</code>内置的Buffer队列中。在Buffer队列中有buffer存在的情况下调用<code>AudioQueueStart</code>，此时<code>AudioQueue</code>就回按照Enqueue顺序逐个使用Buffer队列中的buffer进行播放，每当一个Buffer使用完毕之后就会从Buffer队列中被移除并且在使用者指定的RunLoop上触发一个回调来告诉使用者，某个<code>AudioQueueBufferRef</code>对象已经使用完成，你可以继续重用这个对象来存储后面的音频数据。如此循环往复音频数据就会被逐个播放直到结束。</p>

<p><a href="https://developer.apple.com/library/ios/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/AboutAudioQueues/AboutAudioQueues.html#//apple_ref/doc/uid/TP40005343-CH5-SW9">官方文档</a>给出了一副图来描述这一过程：</p>

<p>其中的callback按我的理解应该是指一个音频数据装填方法，该方法可以通过之前提到的数据使用后的回调来触发。</p>

<p><img src="/images/iOS-audio/audioqueuePlayback.jpg" alt="AudioQueue playback" /></p>

<p>根据Apple提供的<code>AudioQueue</code>工作原理结合自己理解，可以得到其工作流程大致如下：</p>

<ol>
<li>创建<code>AudioQueue</code>，创建一个自己的buffer数组BufferArray;</li>
<li>使用<code>AudioQueueAllocateBuffer</code>创建若干个<code>AudioQueueBufferRef</code>（一般2-3个即可），放入BufferArray；</li>
<li>有数据时从BufferArray取出一个buffer，memcpy数据后用<code>AudioQueueEnqueueBuffer</code>方法把buffer插入<code>AudioQueue</code>中；</li>
<li><code>AudioQueue</code>中存在Buffer后，调用<code>AudioQueueStart</code>播放。（具体等到填入多少buffer后再播放可以自己控制，只要能保证播放不间断即可）；</li>
<li><code>AudioQueue</code>播放音乐后消耗了某个buffer，在另一个线程回调并送出该buffer，把buffer放回BufferArray供下一次使用；</li>
<li>返回步骤3继续循环直到播放结束</li>
</ol>


<p>从以上步骤其实不难看出，<code>AudioQueue</code>播放的过程其实就是一个典型的<a href="http://zh.wikipedia.org/zh/%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E9%97%AE%E9%A2%98">生产者消费者问题</a>。生产者是<code>AudioFileStream</code>或者<code>AudioFile</code>，它们生产处音频数据帧，放入到<code>AudioQueue</code>的buffer队列中，直到buffer填满后需要等待消费者消费；<code>AudioQueue</code>作为消费者，消费了buffer队列中的数据，并且在另一个线程回调通知数据已经被消费生产者可以继续生产。所以在实现<code>AudioQueue</code>播放音频的过程中必然会接触到一些多线程同步、信号量的使用、死锁的避免等等问题。</p>

<p>了解了工作流程之后再回头来看<code>AudioQueue</code>的方法，其中大部分方法都非常好理解，部分需要稍加解释。</p>

<hr />

<h1>创建AudioQueue</h1>

<p>使用下列方法来生成<code>AudioQueue</code>的实例
```objc
OSStatus AudioQueueNewOutput (const AudioStreamBasicDescription * inFormat,</p>

<pre><code>                            AudioQueueOutputCallback inCallbackProc,
                            void * inUserData,
                            CFRunLoopRef inCallbackRunLoop,
                            CFStringRef inCallbackRunLoopMode,
                            UInt32 inFlags,
                            AudioQueueRef * outAQ);
</code></pre>

<p>OSStatus AudioQueueNewOutputWithDispatchQueue(AudioQueueRef * outAQ,</p>

<pre><code>                                            const AudioStreamBasicDescription * inFormat,
                                            UInt32 inFlags,
                                            dispatch_queue_t inCallbackDispatchQueue,
                                            AudioQueueOutputCallbackBlock inCallbackBlock);
</code></pre>

<p>```
先来看第一个方法：</p>

<p>第一个参数表示需要播放的音频数据格式类型，是一个<code>AudioStreamBasicDescription</code>对象，是使用<code>AudioFileStream</code>或者<code>AudioFile</code>解析出来的数据格式信息；</p>

<p>第二个参数<code>AudioQueueOutputCallback</code>是某块Buffer<code>被使用之后</code>的回调；</p>

<p>第三个参数为上下文对象；</p>

<p>第四个参数inCallbackRunLoop为<code>AudioQueueOutputCallback</code>需要在的哪个RunLoop上被回调，如果传入NULL的话就会再<code>AudioQueue</code>的内部RunLoop中被回调，所以一般传NULL就可以了；</p>

<p>第五个参数inCallbackRunLoopMode为RunLoop模式，如果传入NULL就相当于<code>kCFRunLoopCommonModes</code>，也传NULL就可以了；</p>

<p>第六个参数inFlags是保留字段，目前没作用，传0；</p>

<p>第七个参数，返回生成的<code>AudioQueue</code>实例；</p>

<p>返回值用来判断是否成功创建（OSStatus == noErr）。</p>

<p>第二个方法就是把RunLoop替换成了一个dispatch queue，其余参数同相同。</p>

<hr />

<h1>Buffer相关的方法</h1>

<h3>1. 创建Buffer</h3>

<p>```objc
OSStatus AudioQueueAllocateBuffer(AudioQueueRef inAQ,</p>

<pre><code>                                UInt32 inBufferByteSize,
                                AudioQueueBufferRef * outBuffer);
</code></pre>

<p>OSStatus AudioQueueAllocateBufferWithPacketDescriptions(AudioQueueRef inAQ,</p>

<pre><code>                                                      UInt32 inBufferByteSize,
                                                      UInt32 inNumberPacketDescriptions,
                                                      AudioQueueBufferRef * outBuffer);
</code></pre>

<p>```</p>

<p>第一个方法传入<code>AudioQueue</code>实例和Buffer大小，传出的Buffer实例；</p>

<p>第二个方法可以指定生成的Buffer中PacketDescriptions的个数；</p>

<h3>2. 销毁Buffer</h3>

<p><code>objc
OSStatus AudioQueueFreeBuffer(AudioQueueRef inAQ,AudioQueueBufferRef inBuffer);
</code>
注意这个方法一般只在需要销毁特定某个buffer时才会被用到（因为dispose方法会自动销毁所有buffer），并且这个方法<code>只能在AudioQueue不在处理数据时</code>才能使用。所以这个方法一般不太能用到。</p>

<h3>3. 插入Buffer</h3>

<p>```objc
OSStatus AudioQueueEnqueueBuffer(AudioQueueRef inAQ,</p>

<pre><code>                               AudioQueueBufferRef inBuffer,
                               UInt32 inNumPacketDescs,
                               const AudioStreamPacketDescription * inPacketDescs);
</code></pre>

<p><code>``
Enqueue方法一共有两个，上面给出的是第一个方法，第二个方法</code>AudioQueueEnqueueBufferWithParameters`可以对Enqueue的buffer进行更多额外的操作，第二个方法我也没有细细研究，一般来说用第一个方法就能满足需求了，这里我也就只针对第一个方法进行说明：</p>

<p>这个Enqueue方法需要传入<code>AudioQueue</code>实例和需要Enqueue的Buffer，对于有inNumPacketDescs和inPacketDescs则需要根据需要选择传入，文档上说这两个参数主要是在播放VBR数据时使用，但之前我们提到过即便是CBR数据AudioFileStream或者AudioFile也会给出PacketDescription所以不能如此一概而论。简单的来说就是有就传PacketDescription没有就给NULL，不必管是不是VBR。</p>

<hr />

<h1>播放控制</h1>

<h3>1.开始播放</h3>

<p><code>objc
OSStatus AudioQueueStart(AudioQueueRef inAQ,const AudioTimeStamp * inStartTime);
</code></p>

<p>第二个参数可以用来控制播放开始的时间，一般情况下直接开始播放传入NULL即可。</p>

<h3>2.解码数据</h3>

<p>```objc
OSStatus AudioQueuePrime(AudioQueueRef inAQ,</p>

<pre><code>                        UInt32 inNumberOfFramesToPrepare,
                        UInt32 * outNumberOfFramesPrepared);                                    
</code></pre>

<p>```</p>

<p>这个方法并不常用，因为直接调用<code>AudioQueueStart</code>会自动开始解码（如果需要的话）。参数的作用是用来指定需要解码帧数和实际完成解码的帧数；</p>

<h3>3.暂停播放</h3>

<p><code>objc
OSStatus AudioQueuePause(AudioQueueRef inAQ);
</code></p>

<p>需要注意的是这个方法一旦调用后播放就会立即暂停，这就意味着<code>AudioQueueOutputCallback</code>回调也会暂停，这时需要特别关注线程的调度以防止线程陷入无限等待。</p>

<h3>4.停止播放</h3>

<p><code>objc
OSStatus AudioQueueStop(AudioQueueRef inAQ, Boolean inImmediate);
</code></p>

<p>第二个参数如果传入true的话会立即停止播放（同步），如果传入false的话<code>AudioQueue</code>会播放完已经Enqueue的所有buffer后再停止（异步）。使用时注意根据需要传入适合的参数。</p>

<h3>5.Flush</h3>

<p><code>objc
OSStatus
AudioQueueFlush(AudioQueueRef inAQ);
</code></p>

<p>调用后会播放完Enqueu的所有buffer后重置解码器状态，以防止当前的解码器状态影响到下一段音频的解码（比如切换播放的歌曲时）。如果和<code>AudioQueueStop(AQ,false)</code>一起使用并不会起效，因为Stop方法的false参数也会做同样的事情。</p>

<h3>6.重置</h3>

<p><code>objc
OSStatus AudioQueueReset(AudioQueueRef inAQ);
</code>
重置<code>AudioQueue</code>会清除所有已经Enqueue的buffer，并触发<code>AudioQueueOutputCallback</code>,调用<code>AudioQueueStop</code>方法时同样会触发该方法。这个方法的直接调用一般在seek时使用，用来清除残留的buffer（seek时还有一种做法是先<code>AudioQueueStop</code>，等seek完成后重新start）。</p>

<h3>7.获取播放时间</h3>

<p>```objc
OSStatus AudioQueueGetCurrentTime(AudioQueueRef inAQ,</p>

<pre><code>                                AudioQueueTimelineRef inTimeline,
                                AudioTimeStamp * outTimeStamp,
                                Boolean * outTimelineDiscontinuity);
</code></pre>

<p>```</p>

<p>传入的参数中，第一、第四个参数是和<code>AudioQueueTimeline</code>相关的我们这里并没有用到，传入NULL。调用后的返回<code>AudioTimeStamp</code>，从这个timestap结构可以得出播放时间，计算方法如下：</p>

<p><code>objc
AudioTimeStamp time = ...; //AudioQueueGetCurrentTime方法获取
NSTimeInterval playedTime = time.mSampleTime / _format.mSampleRate;
</code></p>

<p>在使用这个时间获取方法时有两点必须注意：</p>

<p>1、 第一个需要注意的时这个播放时间是指<code>实际播放的时间</code>和一般理解上的播放进度是有区别的。举个例子，开始播放8秒后用户操作slider把播放进度seek到了第20秒之后又播放了3秒钟，此时通常意义上播放时间应该是23秒，即播放进度；而用<code>GetCurrentTime</code>方法中获得的时间为11秒，即实际播放时间。所以每次seek时都必须保存seek的timingOffset：</p>

<p>```objc
AudioTimeStamp time = &hellip;; //AudioQueueGetCurrentTime方法获取
NSTimeInterval playedTime = time.mSampleTime / _format.mSampleRate; //seek时的播放时间</p>

<p>NSTimeInterval seekTime = &hellip;; //需要seek到哪个时间
NSTimeInterval timingOffset = seekTime &ndash; playedTime;
```</p>

<p>seek后的播放进度需要根据timingOffset和playedTime计算：
<code>objc
NSTimeInterval progress = timingOffset + playedTime;
</code></p>

<p>2、 第二个需要注意的是<code>GetCurrentTime</code>方法有时候会失败，所以上次获取的播放时间最好保存起来，如果遇到调用失败，就返回上次保存的结果。</p>

<hr />

<h1>销毁AudioQueue</h1>

<p><code>objc
AudioQueueDispose(AudioQueueRef inAQ,  Boolean inImmediate);
</code></p>

<p>销毁的同时会清除其中所有的buffer，第二个参数的意义和用法与<code>AudioQueueStop</code>方法相同。</p>

<p>这个方法使用时需要注意当<code>AudioQueueStart</code>调用之后<code>AudioQueue</code>其实还没有真正开始，期间会有一个短暂的间隙。如果在<code>AudioQueueStart</code>调用后到<code>AudioQueue</code>真正开始运作前的这段时间内调用<code>AudioQueueDispose</code>方法的话会导致程序卡死。这个问题是我在使用<a href="https://github.com/mattgallagher/AudioStreamer">AudioStreamer</a>时发现的，在iOS 6必现（iOS 7我倒是没有测试过，当时发现问题时iOS 7还没发布），起因是由于AudioStreamer会在音频EOF时就进入Cleanup环节，Cleanup环节会flush所有数据然后调用Dispose，那么当音频文件中数据非常少时就有可能出现<code>AudioQueueStart</code>调用之时就已经EOF进入Cleanup，此时就会出现上述问题。</p>

<p>要规避这个问题第一种方法是做好线程的调度，保证Dispose方法调用一定是在每一个播放RunLoop之后（即至少是一个buffer被成功播放之后）。第二种方法是监听<code>kAudioQueueProperty_IsRunning</code>属性，这个属性在<code>AudioQueue</code>真正运作起来之后会变成1，停止后会变成0，所以需要保证Start方法调用后Dispose方法一定要在<code>IsRunning</code>为1时才能被调用。</p>

<hr />

<h1>属性和参数</h1>

<p>和其他的<code>AudioToolBox</code>类一样，<code>AudioToolBox</code>有很多参数和属性可以设置、获取、监听。以下是相关的方法，这里就不再一一赘述：</p>

<p>```objc
//参数相关方法
AudioQueueGetParameter
AudioQueueSetParameter</p>

<p>//属性相关方法
AudioQueueGetPropertySize
AudioQueueGetProperty
AudioQueueSetProperty</p>

<p>//监听属性变化相关方法
AudioQueueAddPropertyListener
AudioQueueRemovePropertyListener
```</p>

<p>属性和参数的列表：
```
//属性列表
enum { // typedef UInt32 AudioQueuePropertyID</p>

<pre><code>kAudioQueueProperty_IsRunning               = 'aqrn',       // value is UInt32

kAudioQueueDeviceProperty_SampleRate        = 'aqsr',       // value is Float64
kAudioQueueDeviceProperty_NumberChannels    = 'aqdc',       // value is UInt32
kAudioQueueProperty_CurrentDevice           = 'aqcd',       // value is CFStringRef

kAudioQueueProperty_MagicCookie             = 'aqmc',       // value is void*
kAudioQueueProperty_MaximumOutputPacketSize = 'xops',       // value is UInt32
kAudioQueueProperty_StreamDescription       = 'aqft',       // value is AudioStreamBasicDescription

kAudioQueueProperty_ChannelLayout           = 'aqcl',       // value is AudioChannelLayout
kAudioQueueProperty_EnableLevelMetering     = 'aqme',       // value is UInt32
kAudioQueueProperty_CurrentLevelMeter       = 'aqmv',       // value is array of AudioQueueLevelMeterState, 1 per channel
kAudioQueueProperty_CurrentLevelMeterDB     = 'aqmd',       // value is array of AudioQueueLevelMeterState, 1 per channel

kAudioQueueProperty_DecodeBufferSizeFrames  = 'dcbf',       // value is UInt32
kAudioQueueProperty_ConverterError          = 'qcve',       // value is UInt32

kAudioQueueProperty_EnableTimePitch         = 'q_tp',       // value is UInt32, 0/1
kAudioQueueProperty_TimePitchAlgorithm      = 'qtpa',       // value is UInt32. See values below.
kAudioQueueProperty_TimePitchBypass         = 'qtpb',       // value is UInt32, 1=bypassed
</code></pre>

<p>};</p>

<p>//参数列表
enum    // typedef UInt32 AudioQueueParameterID;
{</p>

<pre><code>kAudioQueueParam_Volume         = 1,
kAudioQueueParam_PlayRate       = 2,
kAudioQueueParam_Pitch          = 3,
kAudioQueueParam_VolumeRampTime = 4,
kAudioQueueParam_Pan            = 13
</code></pre>

<p>};
```</p>

<p>其中比较有价值的属性有：</p>

<ul>
<li><code>kAudioQueueProperty_IsRunning</code>监听它可以知道当前<code>AudioQueue</code>是否在运行，这个参数的作用在讲到<code>AudioQueueDispose</code>时已经提到过。</li>
<li><code>kAudioQueueProperty_MagicCookie</code>部分音频格式需要设置magicCookie，这个cookie可以从<code>AudioFileStream</code>和<code>AudioFile</code>中获取。</li>
</ul>


<p>比较有价值的参数有：</p>

<ul>
<li><code>kAudioQueueParam_Volume</code>，它可以用来调节<code>AudioQueue</code>的播放音量，注意这个音量是<code>AudioQueue</code>的内部播放音量和系统音量相互独立设置并且最后叠加生效。</li>
<li><code>kAudioQueueParam_VolumeRampTime</code>参数和<code>Volume</code>参数配合使用可以实现音频播放淡入淡出的效果；</li>
<li><code>kAudioQueueParam_PlayRate</code>参数可以调整播放速率；</li>
</ul>


<hr />

<h1>后记</h1>

<p>至此本片关于<code>AudioQueue</code>的话题接结束了。使用上面提到的方法已经可以满足大部分的播放需求，但<code>AudioQueue</code>的功能远不止如此，<code>AudioQueueTimeline</code>、<code>Offline Rendering</code>、<code>AudioQueueProcessingTap</code>等功能我目前也尚未涉及和研究，未来也许还会有更多新的功能加入，学无止境啊。</p>

<p>另外由于<code>AudioQueue</code>的相关内容无法单独做Demo进行展示，于是我提前把后一篇内容的<a href="https://github.com/msching/MCSimpleAudioPlayer">Demo</a>（一个简单的本地音频播放器）先在这里给出方便大家理解<code>AudioQueue</code>。如果觉得上面提到某一部分的很难以的话理解欢迎在下面留言或者在<a href="http://weibo.com/msching">微博</a>上和我交流，除此之外还可以阅读官方文档（我一直觉得官方文档是学习的最好途径）；</p>

<hr />

<h1>示例代码</h1>

<p><a href="https://github.com/mattgallagher/AudioStreamer">AudioStreamer</a>和<a href="https://github.com/muhku/FreeStreamer">FreeStreamer</a>都用到了AudioQueue。</p>

<div class="github-card" data-github="mattgallagher/AudioStreamer" data-width="400" data-height="" data-theme="default"></div>


<script src="//cdn.jsdelivr.net/github-cards/latest/widget.js"></script>




<div class="github-card" data-github="muhku/FreeStreamer" data-width="400" data-height="" data-theme="default"></div>


<script src="//cdn.jsdelivr.net/github-cards/latest/widget.js"></script>


<p>在上面提到的Demo中也有我自己做的封装<a href="https://github.com/msching/MCSimpleAudioPlayer/blob/master/MCSimpleAudioPlayerDemo/MCSimpleAudioPlayer/MCAudioOutputQueue.h">MCAudioOutputQueue</a>。</p>

<div class="github-card" data-github="msching/MCSimpleAudioPlayer" data-width="400" data-height="" data-theme="default"></div>


<script src="//cdn.jsdelivr.net/github-cards/latest/widget.js"></script>


<hr />

<h1>下篇预告</h1>

<p>下一篇将讲述如何利用之前讲到的<code>AudioSession</code>、<code>AudioFileStream</code>和<code>AudioQueue</code>实现一个简单的本地文件播放器。</p>

<hr />

<h1>参考资料</h1>

<p><a href="https://developer.apple.com/library/ios/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/Introduction/Introduction.html#//apple_ref/doc/uid/TP40005343">Audio Queue Services Programming Guide</a></p>

<p><a href="https://developer.apple.com/library/ios/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/AboutAudioQueues/AboutAudioQueues.html#//apple_ref/doc/uid/TP40005343-CH5-SW9">About Audio Queues</a></p>

<p><a href="http://zh.wikipedia.org/zh/%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E9%97%AE%E9%A2%98">生产者消费者问题</a></p>
]]></content>
  </entry>
  
</feed>
